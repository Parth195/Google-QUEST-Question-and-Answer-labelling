{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7968,"databundleVersionId":828965,"sourceType":"competition"},{"sourceId":256023,"sourceType":"datasetVersion","datasetId":107340}],"dockerImageVersionId":29850,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import List, Tuple\nimport random\nimport html\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold, KFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom scipy.optimize import minimize\nfrom math import floor, ceil\nfrom transformers import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef tqdm(it, *args, **kwargs):\n    return it\n\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything()\nnp.set_printoptions(suppress=True)\n\nprint(tf.__version__)\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:30:39.591622Z","iopub.execute_input":"2024-05-04T19:30:39.591999Z","iopub.status.idle":"2024-05-04T19:30:47.909691Z","shell.execute_reply.started":"2024-05-04T19:30:39.591929Z","shell.execute_reply":"2024-05-04T19:30:47.908820Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2.1.0\n1.3.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1. Read data and tokenizer","metadata":{}},{"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\n\nBERT_PATH = '../input/bertpretrained/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\nsub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:30:47.911856Z","iopub.execute_input":"2024-05-04T19:30:47.912218Z","iopub.status.idle":"2024-05-04T19:30:48.402422Z","shell.execute_reply.started":"2024-05-04T19:30:47.912161Z","shell.execute_reply":"2024-05-04T19:30:48.401065Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"train shape = (6079, 41)\ntest shape = (476, 11)\n\noutput categories:\n\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n\ninput categories:\n\t ['question_title', 'question_body', 'answer']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Preprocessing","metadata":{}},{"cell_type":"code","source":"df_train.question_body = df_train.question_body.apply(html.unescape)\ndf_train.question_title = df_train.question_title.apply(html.unescape)\ndf_train.answer = df_train.answer.apply(html.unescape)\n\ndf_test.question_body = df_test.question_body.apply(html.unescape)\ndf_test.question_title = df_test.question_title.apply(html.unescape)\ndf_test.answer = df_test.answer.apply(html.unescape)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:30:48.403989Z","iopub.execute_input":"2024-05-04T19:30:48.404450Z","iopub.status.idle":"2024-05-04T19:30:48.479504Z","shell.execute_reply.started":"2024-05-04T19:30:48.404338Z","shell.execute_reply":"2024-05-04T19:30:48.478798Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def _preprocess_text(s: str) -> str:\n    return s\n\n\ndef _trim_input(question_tokens: List[str], answer_tokens: List[str], max_sequence_length: int, q_max_len: int, a_max_len: int) -> Tuple[List[str], List[str]]:\n    q_len = len(question_tokens)\n    a_len = len(answer_tokens)\n    if q_len + a_len + 3 > max_sequence_length:\n        if a_max_len <= a_len and q_max_len <= q_len:\n            ## Answer も Question も長過ぎる場合、どちらも限界まで切り詰めるしかない\n            q_new_len_head = floor((q_max_len - q_max_len/2))\n            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n            a_new_len_head = floor((a_max_len - a_max_len/2))\n            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n        elif q_len <= a_len and q_len < q_max_len:\n            ## Answer のほうが長く、Question が十分短いなら、その分 Answer にまわす\n            a_max_len = a_max_len + (q_max_len - q_len - 1)\n            a_new_len_head = floor((a_max_len - a_max_len/2))\n            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n        elif a_len < q_len:\n            assert a_len <= a_max_len\n            q_max_len = q_max_len + (a_max_len - a_len - 1)\n            q_new_len_head = floor((q_max_len - q_max_len/2))\n            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n        else:\n            raise ValueError(\"unreachable: q_len: {}, a_len: {}, q_max_len: {}, a_max_len: {}\".format(q_len, a_len, q_max_len, a_max_len))\n    return question_tokens, answer_tokens\n\n\ndef _convert_to_transformer_inputs(title: str, question: str, answer: str, tokenizer: BertTokenizer, question_only=False):\n    title = _preprocess_text(title)\n    question = _preprocess_text(question)\n    answer = _preprocess_text(answer)\n    question = \"{} [SEP] {}\".format(title, question)\n    question_tokens = tokenizer.tokenize(question)\n    if question_only:\n        answer_tokens = []\n    else:\n        answer_tokens = tokenizer.tokenize(answer)\n    question_tokens, answer_tokens = _trim_input(question_tokens, answer_tokens, MAX_SEQUENCE_LENGTH, (MAX_SEQUENCE_LENGTH - 3) // 2, (MAX_SEQUENCE_LENGTH - 3) // 2)\n    ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + question_tokens + [\"[SEP]\"] + answer_tokens + [\"[SEP]\"])\n    padded_ids = ids + [tokenizer.pad_token_id] * (MAX_SEQUENCE_LENGTH - len(ids))\n    token_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(answer_tokens) + 1) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n    attention_mask = [1] * len(ids) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n    return padded_ids, token_type_ids, attention_mask\n\nsample_args = df_train[\"question_title\"].values[0], df_train[\"question_body\"].values[0], df_train[\"answer\"].values[0]\nsample_ids = _convert_to_transformer_inputs(*sample_args, tokenizer, question_only=True)\nprint(sample_ids)\nprint(tokenizer.convert_ids_to_tokens(sample_ids[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:30:48.481317Z","iopub.execute_input":"2024-05-04T19:30:48.481666Z","iopub.status.idle":"2024-05-04T19:30:48.515819Z","shell.execute_reply.started":"2024-05-04T19:30:48.481604Z","shell.execute_reply":"2024-05-04T19:30:48.515037Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"([101, 2054, 2572, 1045, 3974, 2043, 2478, 5331, 10868, 2612, 1997, 1037, 26632, 10014, 1029, 102, 2044, 2652, 2105, 2007, 26632, 5855, 2006, 1011, 1996, 1011, 10036, 1006, 3191, 1024, 11674, 10014, 1010, 7065, 1012, 10014, 5614, 2006, 1037, 3442, 10014, 1010, 13135, 5331, 10868, 1007, 1010, 1045, 2052, 2066, 2000, 2131, 2582, 2007, 2023, 1012, 1996, 3471, 2007, 1996, 5461, 1045, 2109, 2003, 2008, 3579, 2003, 6410, 1998, 18892, 2491, 2003, 18636, 2012, 2190, 1012, 2023, 3132, 2026, 16437, 2000, 2145, 5739, 1006, 3191, 1024, 2757, 9728, 1007, 2085, 1010, 2004, 3500, 2003, 8455, 1010, 1045, 2215, 2000, 2022, 2583, 2000, 5607, 2444, 9728, 1012, 1045, 2903, 2008, 2005, 2023, 1010, 8285, 14876, 7874, 1998, 2275, 10880, 18892, 2097, 2022, 1997, 2307, 2393, 1012, 2061, 1010, 2028, 5793, 2021, 6450, 5724, 2003, 1037, 26632, 10014, 1006, 2360, 1010, 1041, 2546, 2531, 7382, 26632, 1007, 2174, 1010, 1045, 2572, 2025, 2428, 4699, 1999, 2664, 2178, 3539, 10014, 1012, 2019, 4522, 2003, 1996, 5992, 5331, 10868, 1012, 3272, 2005, 4555, 7995, 3292, 1010, 2054, 2572, 1045, 3974, 2043, 2478, 10868, 1006, 11211, 2007, 1037, 2986, 10014, 1010, 2360, 1041, 2546, 19841, 1011, 3263, 1013, 1016, 1012, 1022, 1007, 2612, 1997, 1037, 26632, 10014, 1029, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n['[CLS]', 'what', 'am', 'i', 'losing', 'when', 'using', 'extension', 'tubes', 'instead', 'of', 'a', 'macro', 'lens', '?', '[SEP]', 'after', 'playing', 'around', 'with', 'macro', 'photography', 'on', '-', 'the', '-', 'cheap', '(', 'read', ':', 'reversed', 'lens', ',', 'rev', '.', 'lens', 'mounted', 'on', 'a', 'straight', 'lens', ',', 'passive', 'extension', 'tubes', ')', ',', 'i', 'would', 'like', 'to', 'get', 'further', 'with', 'this', '.', 'the', 'problems', 'with', 'the', 'techniques', 'i', 'used', 'is', 'that', 'focus', 'is', 'manual', 'and', 'aperture', 'control', 'is', 'problematic', 'at', 'best', '.', 'this', 'limited', 'my', 'setup', 'to', 'still', 'subjects', '(', 'read', ':', 'dead', 'insects', ')', 'now', ',', 'as', 'spring', 'is', 'approaching', ',', 'i', 'want', 'to', 'be', 'able', 'to', 'shoot', 'live', 'insects', '.', 'i', 'believe', 'that', 'for', 'this', ',', 'auto', '##fo', '##cus', 'and', 'set', '##table', 'aperture', 'will', 'be', 'of', 'great', 'help', '.', 'so', ',', 'one', 'obvious', 'but', 'expensive', 'option', 'is', 'a', 'macro', 'lens', '(', 'say', ',', 'e', '##f', '100', '##mm', 'macro', ')', 'however', ',', 'i', 'am', 'not', 'really', 'interested', 'in', 'yet', 'another', 'prime', 'lens', '.', 'an', 'alternative', 'is', 'the', 'electrical', 'extension', 'tubes', '.', 'except', 'for', 'maximum', 'focusing', 'distance', ',', 'what', 'am', 'i', 'losing', 'when', 'using', 'tubes', '(', 'coupled', 'with', 'a', 'fine', 'lens', ',', 'say', 'e', '##f', '##70', '-', '200', '/', '2', '.', '8', ')', 'instead', 'of', 'a', 'macro', 'lens', '?', '[SEP]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_input_arrays(df, question_only=False):\n    input_ids, input_token_type_ids, input_attention_masks = [], [], []\n    for title, body, answer in zip(df[\"question_title\"].values, df[\"question_body\"].values, df[\"answer\"].values):\n        ids, type_ids, mask = _convert_to_transformer_inputs(title, body, answer, tokenizer, question_only=question_only)\n        input_ids.append(ids)\n        input_token_type_ids.append(type_ids)\n        input_attention_masks.append(mask)\n    return (\n        np.asarray(input_ids, dtype=np.int32),\n        np.asarray(input_token_type_ids, dtype=np.int32),\n        np.asarray(input_attention_masks, dtype=np.int32),\n    )\n\n\ndef compute_output_arrays(df):\n    return np.asarray(df[output_categories])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:30:48.520169Z","iopub.execute_input":"2024-05-04T19:30:48.520467Z","iopub.status.idle":"2024-05-04T19:30:48.530950Z","shell.execute_reply.started":"2024-05-04T19:30:48.520418Z","shell.execute_reply":"2024-05-04T19:30:48.529831Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 3. Modeling","metadata":{}},{"cell_type":"code","source":"class Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        config = BertConfig.from_json_file(BERT_PATH + \"/bert_config.json\")\n        config.output_hidden_states = True\n        self.bert = BertForPreTraining.from_pretrained(BERT_PATH + \"/bert_model.ckpt.index\", from_tf=True, config=config).bert\n        self.cls_token_head = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(768 * 4, 768),\n            nn.ReLU(inplace=True),\n        )\n        self.qa_sep_token_head = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(768 * 4, 768),\n            nn.ReLU(inplace=True),\n        )\n        self.regressor = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(768 * 2, 30),  # Adjust input size and output size for regression\n        )\n        \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        question_answer_seps = (torch.sum((token_type_ids == 0) * attention_mask, -1) - 1)\n\n#         p_question_answer_dropout = 0.2\n#         if self.training and random.random() < p_question_answer_dropout:\n#             if random.random() < 0.5:\n#                 # mask question\n#                 attention_mask = attention_mask * (token_type_ids == 1)\n#             else:\n#                 # mask answer\n#                 attention_mask = attention_mask * (token_type_ids == 0)\n        \n        _, _, hidden_states = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        hidden_states_cls_embeddings = [x[:, 0] for x in hidden_states[-4:]]\n        x = torch.cat(hidden_states_cls_embeddings, dim=-1)\n        x_cls = self.cls_token_head(x)\n        \n        # Gather [SEP] hidden states\n        tmp = torch.arange(0, len(input_ids), dtype=torch.long)\n        hidden_states_qa_sep_embeddings = [x[tmp, question_answer_seps] for x in hidden_states[-4:]]\n        x = torch.cat(hidden_states_qa_sep_embeddings, dim=-1)\n        \n        x_qa_sep = self.qa_sep_token_head(x)\n        x = torch.cat([x_cls, x_qa_sep], -1)\n        x = self.regressor(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:43:57.061608Z","iopub.execute_input":"2024-05-04T19:43:57.061957Z","iopub.status.idle":"2024-05-04T19:43:57.079775Z","shell.execute_reply.started":"2024-05-04T19:43:57.061907Z","shell.execute_reply":"2024-05-04T19:43:57.078627Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## 4. Training","metadata":{}},{"cell_type":"code","source":"outputs = torch.tensor(compute_output_arrays(df_train), dtype=torch.float)\ninputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train)]\nquestion_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train, question_only=True)]\ntest_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test)]\ntest_question_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test, question_only=True)]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:44:23.469506Z","iopub.execute_input":"2024-05-04T19:44:23.469845Z","iopub.status.idle":"2024-05-04T19:46:26.548692Z","shell.execute_reply.started":"2024-05-04T19:44:23.469797Z","shell.execute_reply":"2024-05-04T19:46:26.547602Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:46:26.551018Z","iopub.execute_input":"2024-05-04T19:46:26.551336Z","iopub.status.idle":"2024-05-04T19:46:26.556772Z","shell.execute_reply.started":"2024-05-04T19:46:26.551277Z","shell.execute_reply":"2024-05-04T19:46:26.555618Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"for n, _ in Model().named_parameters():\n    print(n)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:46:26.558150Z","iopub.execute_input":"2024-05-04T19:46:26.558653Z","iopub.status.idle":"2024-05-04T19:46:30.316478Z","shell.execute_reply.started":"2024-05-04T19:46:26.558432Z","shell.execute_reply":"2024-05-04T19:46:30.315586Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"bert.embeddings.word_embeddings.weight\nbert.embeddings.position_embeddings.weight\nbert.embeddings.token_type_embeddings.weight\nbert.embeddings.LayerNorm.weight\nbert.embeddings.LayerNorm.bias\nbert.encoder.layer.0.attention.self.query.weight\nbert.encoder.layer.0.attention.self.query.bias\nbert.encoder.layer.0.attention.self.key.weight\nbert.encoder.layer.0.attention.self.key.bias\nbert.encoder.layer.0.attention.self.value.weight\nbert.encoder.layer.0.attention.self.value.bias\nbert.encoder.layer.0.attention.output.dense.weight\nbert.encoder.layer.0.attention.output.dense.bias\nbert.encoder.layer.0.attention.output.LayerNorm.weight\nbert.encoder.layer.0.attention.output.LayerNorm.bias\nbert.encoder.layer.0.intermediate.dense.weight\nbert.encoder.layer.0.intermediate.dense.bias\nbert.encoder.layer.0.output.dense.weight\nbert.encoder.layer.0.output.dense.bias\nbert.encoder.layer.0.output.LayerNorm.weight\nbert.encoder.layer.0.output.LayerNorm.bias\nbert.encoder.layer.1.attention.self.query.weight\nbert.encoder.layer.1.attention.self.query.bias\nbert.encoder.layer.1.attention.self.key.weight\nbert.encoder.layer.1.attention.self.key.bias\nbert.encoder.layer.1.attention.self.value.weight\nbert.encoder.layer.1.attention.self.value.bias\nbert.encoder.layer.1.attention.output.dense.weight\nbert.encoder.layer.1.attention.output.dense.bias\nbert.encoder.layer.1.attention.output.LayerNorm.weight\nbert.encoder.layer.1.attention.output.LayerNorm.bias\nbert.encoder.layer.1.intermediate.dense.weight\nbert.encoder.layer.1.intermediate.dense.bias\nbert.encoder.layer.1.output.dense.weight\nbert.encoder.layer.1.output.dense.bias\nbert.encoder.layer.1.output.LayerNorm.weight\nbert.encoder.layer.1.output.LayerNorm.bias\nbert.encoder.layer.2.attention.self.query.weight\nbert.encoder.layer.2.attention.self.query.bias\nbert.encoder.layer.2.attention.self.key.weight\nbert.encoder.layer.2.attention.self.key.bias\nbert.encoder.layer.2.attention.self.value.weight\nbert.encoder.layer.2.attention.self.value.bias\nbert.encoder.layer.2.attention.output.dense.weight\nbert.encoder.layer.2.attention.output.dense.bias\nbert.encoder.layer.2.attention.output.LayerNorm.weight\nbert.encoder.layer.2.attention.output.LayerNorm.bias\nbert.encoder.layer.2.intermediate.dense.weight\nbert.encoder.layer.2.intermediate.dense.bias\nbert.encoder.layer.2.output.dense.weight\nbert.encoder.layer.2.output.dense.bias\nbert.encoder.layer.2.output.LayerNorm.weight\nbert.encoder.layer.2.output.LayerNorm.bias\nbert.encoder.layer.3.attention.self.query.weight\nbert.encoder.layer.3.attention.self.query.bias\nbert.encoder.layer.3.attention.self.key.weight\nbert.encoder.layer.3.attention.self.key.bias\nbert.encoder.layer.3.attention.self.value.weight\nbert.encoder.layer.3.attention.self.value.bias\nbert.encoder.layer.3.attention.output.dense.weight\nbert.encoder.layer.3.attention.output.dense.bias\nbert.encoder.layer.3.attention.output.LayerNorm.weight\nbert.encoder.layer.3.attention.output.LayerNorm.bias\nbert.encoder.layer.3.intermediate.dense.weight\nbert.encoder.layer.3.intermediate.dense.bias\nbert.encoder.layer.3.output.dense.weight\nbert.encoder.layer.3.output.dense.bias\nbert.encoder.layer.3.output.LayerNorm.weight\nbert.encoder.layer.3.output.LayerNorm.bias\nbert.encoder.layer.4.attention.self.query.weight\nbert.encoder.layer.4.attention.self.query.bias\nbert.encoder.layer.4.attention.self.key.weight\nbert.encoder.layer.4.attention.self.key.bias\nbert.encoder.layer.4.attention.self.value.weight\nbert.encoder.layer.4.attention.self.value.bias\nbert.encoder.layer.4.attention.output.dense.weight\nbert.encoder.layer.4.attention.output.dense.bias\nbert.encoder.layer.4.attention.output.LayerNorm.weight\nbert.encoder.layer.4.attention.output.LayerNorm.bias\nbert.encoder.layer.4.intermediate.dense.weight\nbert.encoder.layer.4.intermediate.dense.bias\nbert.encoder.layer.4.output.dense.weight\nbert.encoder.layer.4.output.dense.bias\nbert.encoder.layer.4.output.LayerNorm.weight\nbert.encoder.layer.4.output.LayerNorm.bias\nbert.encoder.layer.5.attention.self.query.weight\nbert.encoder.layer.5.attention.self.query.bias\nbert.encoder.layer.5.attention.self.key.weight\nbert.encoder.layer.5.attention.self.key.bias\nbert.encoder.layer.5.attention.self.value.weight\nbert.encoder.layer.5.attention.self.value.bias\nbert.encoder.layer.5.attention.output.dense.weight\nbert.encoder.layer.5.attention.output.dense.bias\nbert.encoder.layer.5.attention.output.LayerNorm.weight\nbert.encoder.layer.5.attention.output.LayerNorm.bias\nbert.encoder.layer.5.intermediate.dense.weight\nbert.encoder.layer.5.intermediate.dense.bias\nbert.encoder.layer.5.output.dense.weight\nbert.encoder.layer.5.output.dense.bias\nbert.encoder.layer.5.output.LayerNorm.weight\nbert.encoder.layer.5.output.LayerNorm.bias\nbert.encoder.layer.6.attention.self.query.weight\nbert.encoder.layer.6.attention.self.query.bias\nbert.encoder.layer.6.attention.self.key.weight\nbert.encoder.layer.6.attention.self.key.bias\nbert.encoder.layer.6.attention.self.value.weight\nbert.encoder.layer.6.attention.self.value.bias\nbert.encoder.layer.6.attention.output.dense.weight\nbert.encoder.layer.6.attention.output.dense.bias\nbert.encoder.layer.6.attention.output.LayerNorm.weight\nbert.encoder.layer.6.attention.output.LayerNorm.bias\nbert.encoder.layer.6.intermediate.dense.weight\nbert.encoder.layer.6.intermediate.dense.bias\nbert.encoder.layer.6.output.dense.weight\nbert.encoder.layer.6.output.dense.bias\nbert.encoder.layer.6.output.LayerNorm.weight\nbert.encoder.layer.6.output.LayerNorm.bias\nbert.encoder.layer.7.attention.self.query.weight\nbert.encoder.layer.7.attention.self.query.bias\nbert.encoder.layer.7.attention.self.key.weight\nbert.encoder.layer.7.attention.self.key.bias\nbert.encoder.layer.7.attention.self.value.weight\nbert.encoder.layer.7.attention.self.value.bias\nbert.encoder.layer.7.attention.output.dense.weight\nbert.encoder.layer.7.attention.output.dense.bias\nbert.encoder.layer.7.attention.output.LayerNorm.weight\nbert.encoder.layer.7.attention.output.LayerNorm.bias\nbert.encoder.layer.7.intermediate.dense.weight\nbert.encoder.layer.7.intermediate.dense.bias\nbert.encoder.layer.7.output.dense.weight\nbert.encoder.layer.7.output.dense.bias\nbert.encoder.layer.7.output.LayerNorm.weight\nbert.encoder.layer.7.output.LayerNorm.bias\nbert.encoder.layer.8.attention.self.query.weight\nbert.encoder.layer.8.attention.self.query.bias\nbert.encoder.layer.8.attention.self.key.weight\nbert.encoder.layer.8.attention.self.key.bias\nbert.encoder.layer.8.attention.self.value.weight\nbert.encoder.layer.8.attention.self.value.bias\nbert.encoder.layer.8.attention.output.dense.weight\nbert.encoder.layer.8.attention.output.dense.bias\nbert.encoder.layer.8.attention.output.LayerNorm.weight\nbert.encoder.layer.8.attention.output.LayerNorm.bias\nbert.encoder.layer.8.intermediate.dense.weight\nbert.encoder.layer.8.intermediate.dense.bias\nbert.encoder.layer.8.output.dense.weight\nbert.encoder.layer.8.output.dense.bias\nbert.encoder.layer.8.output.LayerNorm.weight\nbert.encoder.layer.8.output.LayerNorm.bias\nbert.encoder.layer.9.attention.self.query.weight\nbert.encoder.layer.9.attention.self.query.bias\nbert.encoder.layer.9.attention.self.key.weight\nbert.encoder.layer.9.attention.self.key.bias\nbert.encoder.layer.9.attention.self.value.weight\nbert.encoder.layer.9.attention.self.value.bias\nbert.encoder.layer.9.attention.output.dense.weight\nbert.encoder.layer.9.attention.output.dense.bias\nbert.encoder.layer.9.attention.output.LayerNorm.weight\nbert.encoder.layer.9.attention.output.LayerNorm.bias\nbert.encoder.layer.9.intermediate.dense.weight\nbert.encoder.layer.9.intermediate.dense.bias\nbert.encoder.layer.9.output.dense.weight\nbert.encoder.layer.9.output.dense.bias\nbert.encoder.layer.9.output.LayerNorm.weight\nbert.encoder.layer.9.output.LayerNorm.bias\nbert.encoder.layer.10.attention.self.query.weight\nbert.encoder.layer.10.attention.self.query.bias\nbert.encoder.layer.10.attention.self.key.weight\nbert.encoder.layer.10.attention.self.key.bias\nbert.encoder.layer.10.attention.self.value.weight\nbert.encoder.layer.10.attention.self.value.bias\nbert.encoder.layer.10.attention.output.dense.weight\nbert.encoder.layer.10.attention.output.dense.bias\nbert.encoder.layer.10.attention.output.LayerNorm.weight\nbert.encoder.layer.10.attention.output.LayerNorm.bias\nbert.encoder.layer.10.intermediate.dense.weight\nbert.encoder.layer.10.intermediate.dense.bias\nbert.encoder.layer.10.output.dense.weight\nbert.encoder.layer.10.output.dense.bias\nbert.encoder.layer.10.output.LayerNorm.weight\nbert.encoder.layer.10.output.LayerNorm.bias\nbert.encoder.layer.11.attention.self.query.weight\nbert.encoder.layer.11.attention.self.query.bias\nbert.encoder.layer.11.attention.self.key.weight\nbert.encoder.layer.11.attention.self.key.bias\nbert.encoder.layer.11.attention.self.value.weight\nbert.encoder.layer.11.attention.self.value.bias\nbert.encoder.layer.11.attention.output.dense.weight\nbert.encoder.layer.11.attention.output.dense.bias\nbert.encoder.layer.11.attention.output.LayerNorm.weight\nbert.encoder.layer.11.attention.output.LayerNorm.bias\nbert.encoder.layer.11.intermediate.dense.weight\nbert.encoder.layer.11.intermediate.dense.bias\nbert.encoder.layer.11.output.dense.weight\nbert.encoder.layer.11.output.dense.bias\nbert.encoder.layer.11.output.LayerNorm.weight\nbert.encoder.layer.11.output.LayerNorm.bias\nbert.pooler.dense.weight\nbert.pooler.dense.bias\ncls_token_head.1.weight\ncls_token_head.1.bias\nqa_sep_token_head.1.weight\nqa_sep_token_head.1.bias\nregressor.1.weight\nregressor.1.bias\n","output_type":"stream"}]},{"cell_type":"code","source":"LABEL_WEIGHTS = torch.tensor(1.0 / df_train[output_categories].std().values, dtype=torch.float32).to(device)\nLABEL_WEIGHTS = LABEL_WEIGHTS / LABEL_WEIGHTS.sum() * 30\nfor name, weight in zip(output_categories, LABEL_WEIGHTS.cpu().numpy()):\n    print(name, \"\\t\", weight)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:46:30.318218Z","iopub.execute_input":"2024-05-04T19:46:30.318602Z","iopub.status.idle":"2024-05-04T19:46:30.333791Z","shell.execute_reply.started":"2024-05-04T19:46:30.318529Z","shell.execute_reply":"2024-05-04T19:46:30.332956Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"question_asker_intent_understanding \t 0.9666648\nquestion_body_critical \t 0.58160704\nquestion_conversational \t 0.7005903\nquestion_expect_short_answer \t 0.36372498\nquestion_fact_seeking \t 0.4212381\nquestion_has_commonly_accepted_answer \t 0.3791943\nquestion_interestingness_others \t 0.9392594\nquestion_interestingness_self \t 0.6863097\nquestion_multi_intent \t 0.3809655\nquestion_not_really_a_question \t 2.7880754\nquestion_opinion_seeking \t 0.34880248\nquestion_type_choice \t 0.346085\nquestion_type_compare \t 0.8308305\nquestion_type_consequence \t 1.7193657\nquestion_type_definition \t 0.92452765\nquestion_type_entity \t 0.646035\nquestion_type_instructions \t 0.3016625\nquestion_type_procedure \t 0.4960922\nquestion_type_reason_explanation \t 0.33294326\nquestion_type_spelling \t 6.230046\nquestion_well_written \t 0.7154175\nanswer_helpful \t 1.1115448\nanswer_level_of_information \t 1.1855657\nanswer_plausible \t 1.4684314\nanswer_relevance \t 1.7103598\nanswer_satisfaction \t 0.97630584\nanswer_type_instructions \t 0.3018177\nanswer_type_procedure \t 0.56550634\nanswer_type_reason_explanation \t 0.3135491\nanswer_well_written \t 1.2674823\n","output_type":"stream"}]},{"cell_type":"code","source":"BEST_BINS = [400, 400, 15, 100, 400, 7, 1600, 100, 100, 400, 100, 9, 8, 50, 9, 8, 15, 400, 400, 5, 400, 400, 800, 50, 200, 1600, 20, 200, 1600, 1600]\n\ndef binning_output(preds, n_bins=BEST_BINS):\n    preds = preds.copy()\n    for i in range(preds.shape[-1]):\n        n = n_bins[i]\n        binned = (preds[:, i] * n).astype(np.int32).astype(np.float32) / n\n        unique_values, unique_counts = np.unique(binned, return_counts=True)\n        # 多数派以外が 0.5 % を下回ったら binning をやめる\n        minor_value_ratio = (unique_counts.sum() - unique_counts.max()) / unique_counts.sum()\n        if minor_value_ratio < 0.005:\n            keep = np.argsort(preds[:, i])[::-1][:int(len(preds) * 0.005) + 1]\n            binned[keep] = preds[keep, i]\n        preds[:, i] = binned\n    return preds\n\n\ndef compute_spearmanr(trues, preds, n_bins=None):\n    rhos = []\n    if n_bins:\n        preds = binning_output(preds, n_bins)\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        if len(np.unique(col_pred)) == 1:\n            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n        rhos.append(spearmanr(col_trues, col_pred).correlation)\n    return np.mean(rhos)\n\n\n\ndef compute_loss(outputs, targets, alpha=0.5, margin=0.1, question_only=False):\n    if question_only:\n        outputs = outputs[:, :21]\n        targets = targets[:, :21]\n    bce = F.binary_cross_entropy_with_logits(outputs, targets, reduction=\"none\")\n    bce = (bce * LABEL_WEIGHTS[:bce.size(-1)]).mean()\n    \n    batch_size = outputs.size(0)\n    if batch_size % 2 == 0:\n        outputs1, outputs2 = outputs.sigmoid().contiguous().view(2, batch_size // 2, outputs.size(-1))\n        targets1, targets2 = targets.contiguous().view(2, batch_size // 2, outputs.size(-1))\n        # 1 if first ones are larger, -1 if second ones are larger, and 0 if equals.\n        ordering = (targets1 > targets2).float() - (targets1 < targets2).float()\n        margin_rank_loss = (-ordering * (outputs1 - outputs2) + margin).clamp(min=0.0)\n        margin_rank_loss = (margin_rank_loss * LABEL_WEIGHTS[:outputs.size(-1)]).mean()\n    else:\n        # batch size is not even number, so we can't devide them into pairs.\n        margin_rank_loss = 0.0\n\n    return alpha * bce + (1 - alpha) * margin_rank_loss\n\n\ndef train_and_predict(train_data, valid_data, test_data, q_train_data, q_valid_data, q_test_data, q_epochs, epochs, batch_size, fold):\n    dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n    valid_dataloader = torch.utils.data.DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n    test_dataloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)\n    q_dataloader = torch.utils.data.DataLoader(q_train_data, shuffle=True, batch_size=batch_size)\n    q_valid_dataloader = torch.utils.data.DataLoader(q_valid_data, shuffle=False, batch_size=batch_size)\n    q_test_dataloader = torch.utils.data.DataLoader(q_test_data, shuffle=False, batch_size=batch_size)\n\n    model = Model().to(device)\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n            \"weight_decay\": 1e-2,\n            \"lr\": 5e-5\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n], \n            \"weight_decay\": 0.0,\n            \"lr\": 5e-5\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n            \"weight_decay\": 1e-2,\n            \"lr\": 5e-4\n            \n        }\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=int(len(dataloader) * (q_epochs) * 0.05),\n        num_training_steps=len(dataloader) * (q_epochs)\n    )\n    \n    test_predictions = []\n    valid_predictions = []\n\n    ## Question Only\n    for epoch in range(q_epochs): \n        import time\n        start = time.time()\n        model.train()\n        train_losses = []\n        train_preds = []\n        train_targets = []\n        for input_ids, token_type_ids, attention_mask, targets in tqdm(q_dataloader, total=len(q_dataloader)):\n            input_ids = input_ids.to(device)\n            token_type_ids = token_type_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            targets = targets.to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n            train_targets.extend(targets.detach().cpu().numpy())\n            loss = compute_loss(outputs, targets, question_only=True)\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses.append(loss.detach().cpu().item())\n        model.eval()\n        valid_losses = []\n        valid_preds = []\n        valid_targets = []\n        with torch.no_grad():\n            for input_ids, token_type_ids, attention_mask, targets in tqdm(q_valid_dataloader, total=len(q_valid_dataloader)):\n                input_ids = input_ids.to(device)\n                token_type_ids = token_type_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                targets = targets.to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                prob = outputs.sigmoid()\n                prob[:, 21:] = 0.0\n                valid_preds.extend(prob.cpu().numpy())\n                valid_targets.extend(targets.cpu().numpy())\n                loss = compute_loss(outputs, targets, question_only=True)\n                valid_losses.append(loss.detach().cpu().item())\n            valid_predictions.append(np.stack(valid_preds))\n            test_preds = []\n            for input_ids, token_type_ids, attention_mask in tqdm(q_test_dataloader, total=len(q_test_dataloader)):\n                input_ids = input_ids.to(device)\n                token_type_ids = token_type_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                prob = outputs.sigmoid()\n                prob[:, 21:] = 0.0\n                test_preds.extend(prob.cpu().numpy())\n            test_predictions.append(np.stack(test_preds))\n            print()\n        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) / len(valid_predictions)),\n            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n        ))\n        print(\"\\t elapsed: {}s\".format(time.time() - start))\n\n    ## Q and A\n    model = Model().to(device)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n            \"weight_decay\": 1e-2,\n            \"lr\": 5e-5\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n], \n            \"weight_decay\": 0.0,\n            \"lr\": 5e-5\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n            \"weight_decay\": 1e-2,\n            \"lr\": 5e-4\n            \n        }\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=int(len(dataloader) * (epochs) * 0.05),\n        num_training_steps=len(dataloader) * (epochs)\n    )\n\n    for epoch in range(epochs): \n        import time\n        start = time.time()\n        model.train()\n        train_losses = []\n        train_preds = []\n        train_targets = []\n        for input_ids, token_type_ids, attention_mask, targets in tqdm(dataloader, total=len(dataloader)):\n            input_ids = input_ids.to(device)\n            token_type_ids = token_type_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            targets = targets.to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n            train_targets.extend(targets.detach().cpu().numpy())\n            loss = compute_loss(outputs, targets)\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses.append(loss.detach().cpu().item())\n        model.eval()\n        valid_losses = []\n        valid_preds = []\n        valid_targets = []\n        with torch.no_grad():\n            for input_ids, token_type_ids, attention_mask, targets in tqdm(valid_dataloader, total=len(valid_dataloader)):\n                input_ids = input_ids.to(device)\n                token_type_ids = token_type_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                targets = targets.to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                valid_preds.extend(outputs.sigmoid().cpu().numpy())\n                valid_targets.extend(targets.cpu().numpy())\n                loss = compute_loss(outputs, targets)\n                valid_losses.append(loss.detach().cpu().item())\n            valid_predictions.append(np.stack(valid_preds))\n            test_preds = []\n            for input_ids, token_type_ids, attention_mask in tqdm(test_dataloader, total=len(test_dataloader)):\n                input_ids = input_ids.to(device)\n                token_type_ids = token_type_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                test_preds.extend(outputs.sigmoid().cpu().numpy())\n            test_predictions.append(np.stack(test_preds))\n            print()\n        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) / len(valid_predictions)),\n            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n        ))\n        print(\"\\t elapsed: {}s\".format(time.time() - start))\n\n    return valid_predictions, test_predictions","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:46:30.339593Z","iopub.execute_input":"2024-05-04T19:46:30.339858Z","iopub.status.idle":"2024-05-04T19:46:30.426171Z","shell.execute_reply.started":"2024-05-04T19:46:30.339817Z","shell.execute_reply":"2024-05-04T19:46:30.425411Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class Fold(object):\n    def __init__(self, n_splits=5, shuffle=True, random_state=71):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_groupkfold(self, train, group_name):\n        group = train[group_name]\n        unique_group = group.unique()\n\n        kf = KFold(\n            n_splits=self.n_splits,\n            shuffle=self.shuffle,\n            random_state=self.random_state\n        )\n        folds_ids = []\n        for trn_group_idx, val_group_idx in kf.split(unique_group):\n            trn_group = unique_group[trn_group_idx]\n            val_group = unique_group[val_group_idx]\n            is_trn = group.isin(trn_group)\n            is_val = group.isin(val_group)\n            trn_idx = train[is_trn].index\n            val_idx = train[is_val].index\n            folds_ids.append((trn_idx, val_idx))\n\n        return folds_ids","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:46:30.428104Z","iopub.execute_input":"2024-05-04T19:46:30.428395Z","iopub.status.idle":"2024-05-04T19:46:30.441442Z","shell.execute_reply.started":"2024-05-04T19:46:30.428343Z","shell.execute_reply":"2024-05-04T19:46:30.440547Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"gkf = Fold(n_splits=3, shuffle=True, random_state=71)\nfold_ids = gkf.get_groupkfold(df_train, group_name=\"url\")\n\nfor train_idx, valid_idx in fold_ids:\n    print((df_train.loc[train_idx, \"question_type_spelling\"] > 0).sum())\n    print((df_train.loc[valid_idx, \"question_type_spelling\"] > 0).sum())","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:46:30.442759Z","iopub.execute_input":"2024-05-04T19:46:30.443023Z","iopub.status.idle":"2024-05-04T19:46:30.498187Z","shell.execute_reply.started":"2024-05-04T19:46:30.442977Z","shell.execute_reply":"2024-05-04T19:46:30.496842Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"6\n5\n6\n5\n10\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"histories = []\ntest_dataset = torch.utils.data.TensorDataset(*test_inputs)\nq_test_dataset = torch.utils.data.TensorDataset(*test_question_only_inputs)\n\nfor fold, (train_idx, valid_idx) in enumerate(fold_ids):\n    import gc\n    gc.collect()\n\n    train_inputs = [inputs[i][train_idx] for i in range(3)]\n    q_train_inputs = [question_only_inputs[i][train_idx] for i in range(3)]\n    train_outputs = outputs[train_idx]\n    train_dataset = torch.utils.data.TensorDataset(*train_inputs, train_outputs)\n    q_train_dataset = torch.utils.data.TensorDataset(*q_train_inputs, train_outputs)\n\n    valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n    q_valid_inputs = [question_only_inputs[i][valid_idx] for i in range(3)]\n    valid_outputs = outputs[valid_idx]\n    valid_dataset = torch.utils.data.TensorDataset(*valid_inputs, valid_outputs)\n    q_valid_dataset = torch.utils.data.TensorDataset(*q_valid_inputs, valid_outputs)\n\n    history = train_and_predict(\n        train_data=train_dataset, \n        valid_data=valid_dataset,\n        test_data=test_dataset, \n        q_train_data=q_train_dataset, \n        q_valid_data=q_valid_dataset,\n        q_test_data=q_test_dataset, \n        q_epochs=3, epochs=3, batch_size=8, fold=fold\n        )\n\n    histories.append(history)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:46:30.500340Z","iopub.execute_input":"2024-05-04T19:46:30.500675Z","iopub.status.idle":"2024-05-04T21:13:39.773294Z","shell.execute_reply.started":"2024-05-04T19:46:30.500614Z","shell.execute_reply":"2024-05-04T21:13:39.772126Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\nEpoch 1: Train Loss 0.1613324142933365, Valid Loss 0.14961062783397125\n\t Train Spearmanr 0.2039, Valid Spearmanr (avg) 0.2832, Valid Spearmanr (last) 0.2870\n\t elapsed: 288.0481553077698s\n\nEpoch 2: Train Loss 0.14568980063296974, Valid Loss 0.147496070031526\n\t Train Spearmanr 0.2922, Valid Spearmanr (avg) 0.3009, Valid Spearmanr (last) 0.2929\n\t elapsed: 288.0889048576355s\n\nEpoch 3: Train Loss 0.13933569192886353, Valid Loss 0.14678888046092098\n\t Train Spearmanr 0.3328, Valid Spearmanr (avg) 0.3005, Valid Spearmanr (last) 0.2969\n\t elapsed: 287.9650809764862s\n\nEpoch 1: Train Loss 0.17802272045186587, Valid Loss 0.16625460860562233\n\t Train Spearmanr 0.2644, Valid Spearmanr (avg) 0.3994, Valid Spearmanr (last) 0.3771\n\t elapsed: 287.91700887680054s\n\nEpoch 2: Train Loss 0.1629135919113954, Valid Loss 0.16368043567651905\n\t Train Spearmanr 0.4096, Valid Spearmanr (avg) 0.4103, Valid Spearmanr (last) 0.3967\n\t elapsed: 287.9059839248657s\n\nEpoch 3: Train Loss 0.15489576974262795, Valid Loss 0.16374141493891928\n\t Train Spearmanr 0.4986, Valid Spearmanr (avg) 0.4127, Valid Spearmanr (last) 0.4033\n\t elapsed: 287.87364315986633s\n\nEpoch 1: Train Loss 0.16155060539407842, Valid Loss 0.15054321937321677\n\t Train Spearmanr 0.2093, Valid Spearmanr (avg) 0.2908, Valid Spearmanr (last) 0.2828\n\t elapsed: 289.1746492385864s\n\nEpoch 2: Train Loss 0.14594085757019956, Valid Loss 0.1486105171772908\n\t Train Spearmanr 0.2999, Valid Spearmanr (avg) 0.2991, Valid Spearmanr (last) 0.3049\n\t elapsed: 289.1493983268738s\n\nEpoch 3: Train Loss 0.13946717574869152, Valid Loss 0.14748439430369167\n\t Train Spearmanr 0.3471, Valid Spearmanr (avg) 0.3036, Valid Spearmanr (last) 0.3047\n\t elapsed: 289.29045248031616s\n\nEpoch 1: Train Loss 0.1772611909775574, Valid Loss 0.16787448073700656\n\t Train Spearmanr 0.2659, Valid Spearmanr (avg) 0.4085, Valid Spearmanr (last) 0.3816\n\t elapsed: 288.97467160224915s\n\nEpoch 2: Train Loss 0.16255290286075433, Valid Loss 0.1655469983229487\n\t Train Spearmanr 0.4042, Valid Spearmanr (avg) 0.4186, Valid Spearmanr (last) 0.4056\n\t elapsed: 288.9174382686615s\n\nEpoch 3: Train Loss 0.15463513091823758, Valid Loss 0.16459552914373518\n\t Train Spearmanr 0.4935, Valid Spearmanr (avg) 0.4210, Valid Spearmanr (last) 0.4113\n\t elapsed: 288.9337434768677s\n\nEpoch 1: Train Loss 0.16158821102158696, Valid Loss 0.1507686598598957\n\t Train Spearmanr 0.2310, Valid Spearmanr (avg) 0.2771, Valid Spearmanr (last) 0.2754\n\t elapsed: 290.03732419013977s\n\nEpoch 2: Train Loss 0.14520661169115234, Valid Loss 0.14868836146593095\n\t Train Spearmanr 0.3166, Valid Spearmanr (avg) 0.2871, Valid Spearmanr (last) 0.2805\n\t elapsed: 290.20555686950684s\n\nEpoch 3: Train Loss 0.1391587575861052, Valid Loss 0.1482025186419487\n\t Train Spearmanr 0.3627, Valid Spearmanr (avg) 0.2888, Valid Spearmanr (last) 0.2828\n\t elapsed: 290.2080841064453s\n\nEpoch 1: Train Loss 0.17748937986645044, Valid Loss 0.16694339162111282\n\t Train Spearmanr 0.2695, Valid Spearmanr (avg) 0.3951, Valid Spearmanr (last) 0.3670\n\t elapsed: 289.89291310310364s\n\nEpoch 2: Train Loss 0.16207920165330755, Valid Loss 0.16646714824438094\n\t Train Spearmanr 0.4156, Valid Spearmanr (avg) 0.4023, Valid Spearmanr (last) 0.3879\n\t elapsed: 290.05344557762146s\n\nEpoch 3: Train Loss 0.1543653414091643, Valid Loss 0.16493227183818818\n\t Train Spearmanr 0.4984, Valid Spearmanr (avg) 0.4047, Valid Spearmanr (last) 0.3948\n\t elapsed: 290.0079302787781s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5. Submit","metadata":{}},{"cell_type":"code","source":"# get val preds per each epochs\nval_preds_list = []\nn_epochs = len(histories[0][0])\n\nfor epoch in range(n_epochs):\n    val_preds_one_epoch = np.zeros([len(df_train), 30])    \n\n    for fold, (train_idx, valid_idx) in enumerate(fold_ids):\n        val_pred = histories[fold][0][epoch]\n        val_preds_one_epoch[valid_idx, :] += val_pred\n\n    val_preds_list.append(val_preds_one_epoch)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:13:39.775865Z","iopub.execute_input":"2024-05-04T21:13:39.776111Z","iopub.status.idle":"2024-05-04T21:13:39.790122Z","shell.execute_reply.started":"2024-05-04T21:13:39.776069Z","shell.execute_reply":"2024-05-04T21:13:39.789062Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"oof_predictions = np.zeros((n_epochs, len(df_train), len(output_categories)), dtype=np.float32)\n\nfor j, name in enumerate(output_categories):\n    for epoch in range(n_epochs):\n        col = \"{}_{}\".format(epoch, name)\n        oof_predictions[epoch, :, j] = val_preds_list[epoch][:, j]\n\noof_predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:13:39.791777Z","iopub.execute_input":"2024-05-04T21:13:39.792299Z","iopub.status.idle":"2024-05-04T21:13:39.813505Z","shell.execute_reply.started":"2024-05-04T21:13:39.792078Z","shell.execute_reply":"2024-05-04T21:13:39.812825Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(6, 6079, 30)"},"metadata":{}}]},{"cell_type":"code","source":"# get test preds per each epochs\ntest_preds_list = []\n\nfor epoch in range(n_epochs):\n    test_preds_one_epoch = 0\n\n    for fold in range(len(fold_ids)):\n        test_preds = histories[fold][1][epoch]\n        test_preds_one_epoch += test_preds\n\n    test_preds_one_epoch = test_preds_one_epoch / len(fold_ids)\n    test_preds_list.append(test_preds_one_epoch)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:13:39.814785Z","iopub.execute_input":"2024-05-04T21:13:39.815007Z","iopub.status.idle":"2024-05-04T21:13:39.821823Z","shell.execute_reply.started":"2024-05-04T21:13:39.814970Z","shell.execute_reply":"2024-05-04T21:13:39.820965Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"test_predictions = np.zeros((n_epochs, len(df_test), len(output_categories)), dtype=np.float32)\n\nfor j, name in enumerate(output_categories):\n    for epoch in range(n_epochs):\n        col = \"{}_{}\".format(epoch, name)\n        test_predictions[epoch, :, j] = test_preds_list[epoch][:, j]\n\ntest_predictions.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:13:39.823624Z","iopub.execute_input":"2024-05-04T21:13:39.823955Z","iopub.status.idle":"2024-05-04T21:13:39.835756Z","shell.execute_reply.started":"2024-05-04T21:13:39.823891Z","shell.execute_reply":"2024-05-04T21:13:39.834957Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(6, 476, 30)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom abc import abstractmethod\nfrom sklearn.metrics import roc_auc_score\n\n\nclass Base_Model(object):\n    @abstractmethod\n    def fit(self, x_train, y_train, x_valid, y_valid, config):\n        raise NotImplementedError\n    \n    @abstractmethod\n    def get_best_iteration(self, model):\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, model, features):\n        raise NotImplementedError\n        \n    @abstractmethod\n    def get_feature_importance(self, model):\n        raise NotImplementedError      \n        \n\n    def cv(self, y_train, train_features, test_features, feature_name, folds_ids, config):\n        # initialize\n        test_preds = np.zeros(len(test_features))\n        oof_preds = np.zeros(len(train_features))\n        importances = pd.DataFrame(index=feature_name)\n        best_iteration = 0\n        cv_score_list = []\n        models = []\n\n        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n            # get train data and valid data\n            x_trn = train_features.iloc[trn_idx]\n            y_trn = y_train[trn_idx]\n            x_val = train_features.iloc[val_idx]\n            y_val = y_train[val_idx]\n            \n            # train model\n            model, best_score = self.fit(x_trn, y_trn, x_val, y_val, config)\n            cv_score_list.append(best_score)\n            models.append(model)\n            best_iteration += self.get_best_iteration(model) / len(folds_ids)\n    \n            # predict out-of-fold and test\n            oof_preds[val_idx] = self.predict(model, x_val)\n            test_preds += self.predict(model, test_features) / len(folds_ids)\n\n            # get feature importances\n            importances_tmp = pd.DataFrame(\n                self.get_feature_importance(model),\n                columns=[f'gain_{i_fold+1}'],\n                index=feature_name\n            )\n            importances = importances.join(importances_tmp, how='inner')\n\n        # summary of feature importance\n        feature_importance = importances.mean(axis=1)\n\n        # full train\n        # model, best_score = self.full_train(train_features, y_train, config, best_iteration * 1.5)\n        # oof_preds = self.predict(model, train_features)\n        # test_preds = self.predict(model, test_features)\n    \n        evals_results = {\"evals_result\": {\n            \"cv_score\": {f\"cv{i+1}\": cv_score for i, cv_score in enumerate(cv_score_list)},\n            \"n_data\": len(train_features),\n            \"best_iteration\": best_iteration,\n            \"n_features\": len(train_features.columns),\n            \"feature_importance\": feature_importance.sort_values(ascending=False).to_dict()\n        }}\n\n        return models, oof_preds, test_preds, feature_importance, evals_results","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:13:39.837481Z","iopub.execute_input":"2024-05-04T21:13:39.837788Z","iopub.status.idle":"2024-05-04T21:13:39.857620Z","shell.execute_reply.started":"2024-05-04T21:13:39.837741Z","shell.execute_reply":"2024-05-04T21:13:39.856726Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def lgb_compute_spearmanr(preds, trues):\n    rhos = spearmanr(trues.get_label(), preds).correlation\n    return \"spearmanr\", rhos, True\n\n\ndef compute_spearmanr_each_col(trues, preds, n_bins=None):\n    if n_bins:\n        preds = binning_output(preds, n_bins)\n    rhos = spearmanr(trues, preds).correlation\n    return rhos","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:13:39.858974Z","iopub.execute_input":"2024-05-04T21:13:39.859268Z","iopub.status.idle":"2024-05-04T21:13:39.869383Z","shell.execute_reply.started":"2024-05-04T21:13:39.859205Z","shell.execute_reply":"2024-05-04T21:13:39.868602Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nfrom pathlib import Path\n\n\nclass LightGBM(Base_Model):\n    def fit(self, x_train, y_train, x_valid, y_valid, config):\n        d_train = lgb.Dataset(x_train, label=y_train)\n        d_valid = lgb.Dataset(x_valid, label=y_valid)\n        lgb_model_params = config[\"model\"][\"model_params\"]\n        lgb_train_params = config[\"model\"][\"train_params\"]\n        model = lgb.train(\n            params=lgb_model_params,\n            train_set=d_train,\n            valid_sets=[d_valid],\n            valid_names=['valid'],\n            feval=lgb_compute_spearmanr,\n            **lgb_train_params\n        )\n        best_score = dict(model.best_score)\n        return model, best_score\n\n    def full_train(self, x_train, y_train, config, iteration):\n        d_train = lgb.Dataset(x_train, label=y_train)\n        lgb_model_params = config[\"model\"][\"model_params\"]\n        model = lgb.train(\n            params=lgb_model_params,\n            train_set=d_train,\n            feval=lgb_compute_spearmanr,\n            num_boost_round=int(iteration)\n        )\n        best_score = dict(model.best_score)\n        return model, best_score\n\n    def get_best_iteration(self, model):\n        return model.best_iteration\n    \n    def predict(self, model, features):\n        return model.predict(features)\n        \n    def get_feature_importance(self, model):\n        return model.feature_importance(importance_type='gain')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:13:39.870949Z","iopub.execute_input":"2024-05-04T21:13:39.871370Z","iopub.status.idle":"2024-05-04T21:13:39.930220Z","shell.execute_reply.started":"2024-05-04T21:13:39.871312Z","shell.execute_reply":"2024-05-04T21:13:39.929138Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"model\": {\n        \"name\": \"lightgbm\",\n        \"model_params\": {\n            \"boosting_type\": \"gbdt\",\n            \"objective\": \"rmse\",\n            \"tree_learner\": \"serial\",\n            \"learning_rate\": 0.1,\n            \"max_depth\": 1,\n            \"seed\": 71,\n            \"bagging_seed\": 71,\n            \"feature_fraction_seed\": 71,\n            \"drop_seed\": 71,\n            \"verbose\": -1\n        },\n        \"train_params\": {\n            \"num_boost_round\": 5000,\n            \"early_stopping_rounds\": 200,\n            \"verbose_eval\": 500\n        }\n    }\n}\n\n\noutputs = compute_output_arrays(df_train)\noof_preds_list = []\ntest_preds_list = []\n\nfor i_col in range(len(output_categories)):\n    y_train = outputs[:, i_col]\n    #x_train = pd.DataFrame(oof_predictions[:, :, 2].T)\n    x_train = pd.DataFrame(np.concatenate([oof_predictions[:, :, i].T for i in range(30)], axis=1))\n    x_test = pd.DataFrame(np.concatenate([test_predictions[:, :, i].T for i in range(30)], axis=1))\n    feature_name = x_train.columns\n\n    model = LightGBM()\n    models, oof_preds, test_preds, feature_importance, evals_results = model.cv(\n            y_train, x_train, x_test, feature_name, fold_ids, config\n    )\n    oof_preds_list.append(oof_preds.reshape(-1, 1))\n    test_preds_list.append(test_preds.reshape(-1, 1))\n\n    print(i_col, output_categories[i_col])\n    print(compute_spearmanr_each_col(oof_preds, y_train))\n    print(len(oof_preds), len(np.unique(oof_preds)))\n    print(len(test_preds), len(np.unique(test_preds)))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:13:39.931818Z","iopub.execute_input":"2024-05-04T21:13:39.932143Z","iopub.status.idle":"2024-05-04T21:14:53.179685Z","shell.execute_reply.started":"2024-05-04T21:13:39.932089Z","shell.execute_reply":"2024-05-04T21:14:53.178809Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[30]\tvalid's rmse: 0.124295\tvalid's spearmanr: 0.358147\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[48]\tvalid's rmse: 0.123747\tvalid's spearmanr: 0.400683\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[21]\tvalid's rmse: 0.124589\tvalid's spearmanr: 0.388652\n0 question_asker_intent_understanding\n0.3726024121295466\n6079 1894\n476 448\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[31]\tvalid's rmse: 0.170817\tvalid's spearmanr: 0.653621\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[33]\tvalid's rmse: 0.167154\tvalid's spearmanr: 0.657628\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[39]\tvalid's rmse: 0.173739\tvalid's spearmanr: 0.618575\n1 question_body_critical\n0.6324748870826311\n6079 907\n476 291\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[6]\tvalid's rmse: 0.157448\tvalid's spearmanr: 0.516992\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2]\tvalid's rmse: 0.180753\tvalid's spearmanr: 0.510991\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[10]\tvalid's rmse: 0.164565\tvalid's spearmanr: 0.460996\n2 question_conversational\n0.33984248394893257\n6079 44\n476 16\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[182]\tvalid's rmse: 0.335445\tvalid's spearmanr: 0.32209\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[20]\tvalid's rmse: 0.334683\tvalid's spearmanr: 0.286996\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[22]\tvalid's rmse: 0.33492\tvalid's spearmanr: 0.291608\n3 question_expect_short_answer\n0.2911537657136908\n6079 2148\n476 474\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[33]\tvalid's rmse: 0.26782\tvalid's spearmanr: 0.388582\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[39]\tvalid's rmse: 0.276717\tvalid's spearmanr: 0.364258\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[4]\tvalid's rmse: 0.284694\tvalid's spearmanr: 0.328158\n4 question_fact_seeking\n0.3152587246258017\n6079 740\n476 230\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[4]\tvalid's rmse: 0.312092\tvalid's spearmanr: 0.466024\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[5]\tvalid's rmse: 0.320084\tvalid's spearmanr: 0.51318\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[10]\tvalid's rmse: 0.289152\tvalid's spearmanr: 0.472634\n5 question_has_commonly_accepted_answer\n0.396408822527907\n6079 64\n476 39\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[18]\tvalid's rmse: 0.126975\tvalid's spearmanr: 0.35627\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[36]\tvalid's rmse: 0.129562\tvalid's spearmanr: 0.401839\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[12]\tvalid's rmse: 0.124318\tvalid's spearmanr: 0.336116\n6 question_interestingness_others\n0.35562613867082754\n6079 1177\n476 268\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[10]\tvalid's rmse: 0.164651\tvalid's spearmanr: 0.505607\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[30]\tvalid's rmse: 0.165845\tvalid's spearmanr: 0.538719\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[59]\tvalid's rmse: 0.157656\tvalid's spearmanr: 0.493687\n7 question_interestingness_self\n0.48794155293897257\n6079 1530\n476 311\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[17]\tvalid's rmse: 0.271374\tvalid's spearmanr: 0.590978\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[9]\tvalid's rmse: 0.298462\tvalid's spearmanr: 0.56511\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[11]\tvalid's rmse: 0.28448\tvalid's spearmanr: 0.590338\n8 question_multi_intent\n0.5465834821308226\n6079 76\n476 54\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[5]\tvalid's rmse: 0.0419221\tvalid's spearmanr: 0.0994508\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[4]\tvalid's rmse: 0.0520178\tvalid's spearmanr: 0.101968\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[45]\tvalid's rmse: 0.0423528\tvalid's spearmanr: 0.111804\n9 question_not_really_a_question\n0.06975546656374386\n6079 72\n476 45\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[47]\tvalid's rmse: 0.322435\tvalid's spearmanr: 0.461177\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[47]\tvalid's rmse: 0.313297\tvalid's spearmanr: 0.514966\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[36]\tvalid's rmse: 0.325355\tvalid's spearmanr: 0.435931\n10 question_opinion_seeking\n0.4647548303078881\n6079 1608\n476 383\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[11]\tvalid's rmse: 0.267216\tvalid's spearmanr: 0.77037\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[21]\tvalid's rmse: 0.260811\tvalid's spearmanr: 0.767193\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[10]\tvalid's rmse: 0.277416\tvalid's spearmanr: 0.746527\n11 question_type_choice\n0.7123824886157509\n6079 219\n476 100\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[3]\tvalid's rmse: 0.13745\tvalid's spearmanr: 0.52649\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[8]\tvalid's rmse: 0.134078\tvalid's spearmanr: 0.532836\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[5]\tvalid's rmse: 0.134613\tvalid's spearmanr: 0.468892\n12 question_type_compare\n0.2281404090669792\n6079 26\n476 10\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[6]\tvalid's rmse: 0.0774368\tvalid's spearmanr: 0.335941\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[12]\tvalid's rmse: 0.0739831\tvalid's spearmanr: 0.27463\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[5]\tvalid's rmse: 0.0628351\tvalid's spearmanr: 0.261532\n13 question_type_consequence\n0.1349476029588603\n6079 47\n476 18\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[3]\tvalid's rmse: 0.11583\tvalid's spearmanr: 0.594258\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[10]\tvalid's rmse: 0.114009\tvalid's spearmanr: 0.6777\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.137102\tvalid's spearmanr: 0.598749\n14 question_type_definition\n0.2873237665666397\n6079 33\n476 15\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[12]\tvalid's rmse: 0.162201\tvalid's spearmanr: 0.591602\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[7]\tvalid's rmse: 0.165505\tvalid's spearmanr: 0.581832\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[6]\tvalid's rmse: 0.171287\tvalid's spearmanr: 0.57914\n15 question_type_entity\n0.38392239025356684\n6079 34\n476 27\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[12]\tvalid's rmse: 0.283201\tvalid's spearmanr: 0.795452\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[6]\tvalid's rmse: 0.321109\tvalid's spearmanr: 0.782047\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[13]\tvalid's rmse: 0.278008\tvalid's spearmanr: 0.78663\n16 question_type_instructions\n0.7334709886944727\n6079 150\n476 83\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[26]\tvalid's rmse: 0.240965\tvalid's spearmanr: 0.337256\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[18]\tvalid's rmse: 0.238601\tvalid's spearmanr: 0.366902\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[10]\tvalid's rmse: 0.252638\tvalid's spearmanr: 0.365307\n17 question_type_procedure\n0.33647516916849973\n6079 341\n476 156\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[45]\tvalid's rmse: 0.275167\tvalid's spearmanr: 0.689702\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[21]\tvalid's rmse: 0.281084\tvalid's spearmanr: 0.686284\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[13]\tvalid's rmse: 0.298025\tvalid's spearmanr: 0.65678\n18 question_type_reason_explanation\n0.6618717287317771\n6079 552\n476 229\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.0205601\tvalid's spearmanr: 0.259633\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.0243297\tvalid's spearmanr: 0.546413\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.0147291\tvalid's spearmanr: 0.28788\n19 question_type_spelling\n0.046761580615449286\n6079 6\n476 3\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[35]\tvalid's rmse: 0.149093\tvalid's spearmanr: 0.523263\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[25]\tvalid's rmse: 0.154422\tvalid's spearmanr: 0.533499\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[42]\tvalid's rmse: 0.153183\tvalid's spearmanr: 0.510179\n20 question_well_written\n0.514223995240751\n6079 1858\n476 383\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[21]\tvalid's rmse: 0.107657\tvalid's spearmanr: 0.223462\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[40]\tvalid's rmse: 0.112614\tvalid's spearmanr: 0.271836\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[19]\tvalid's rmse: 0.112913\tvalid's spearmanr: 0.239732\n21 answer_helpful\n0.242181206014872\n6079 642\n476 258\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[38]\tvalid's rmse: 0.0963434\tvalid's spearmanr: 0.422762\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[41]\tvalid's rmse: 0.100065\tvalid's spearmanr: 0.426835\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[62]\tvalid's rmse: 0.0990761\tvalid's spearmanr: 0.425644\n22 answer_level_of_information\n0.40860876029817567\n6079 1766\n476 415\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[27]\tvalid's rmse: 0.0836987\tvalid's spearmanr: 0.144109\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[24]\tvalid's rmse: 0.0886355\tvalid's spearmanr: 0.171466\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[14]\tvalid's rmse: 0.0851798\tvalid's spearmanr: 0.176386\n23 answer_plausible\n0.15294265048984748\n6079 383\n476 197\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[4]\tvalid's rmse: 0.072711\tvalid's spearmanr: 0.190732\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2]\tvalid's rmse: 0.0772422\tvalid's spearmanr: 0.196407\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[11]\tvalid's rmse: 0.0704829\tvalid's spearmanr: 0.21531\n24 answer_relevance\n0.17250068613611913\n6079 34\n476 26\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[19]\tvalid's rmse: 0.121819\tvalid's spearmanr: 0.297112\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[38]\tvalid's rmse: 0.125134\tvalid's spearmanr: 0.335384\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[34]\tvalid's rmse: 0.121295\tvalid's spearmanr: 0.332472\n25 answer_satisfaction\n0.31071460322866884\n6079 1359\n476 371\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[34]\tvalid's rmse: 0.273364\tvalid's spearmanr: 0.76247\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[27]\tvalid's rmse: 0.27837\tvalid's spearmanr: 0.757308\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[16]\tvalid's rmse: 0.290889\tvalid's spearmanr: 0.749923\n26 answer_type_instructions\n0.7447123503058137\n6079 479\n476 204\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[39]\tvalid's rmse: 0.218791\tvalid's spearmanr: 0.293425\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[18]\tvalid's rmse: 0.218699\tvalid's spearmanr: 0.289062\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[28]\tvalid's rmse: 0.217536\tvalid's spearmanr: 0.294675\n27 answer_type_procedure\n0.27718279705238447\n6079 619\n476 299\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[31]\tvalid's rmse: 0.305243\tvalid's spearmanr: 0.68142\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[37]\tvalid's rmse: 0.298837\tvalid's spearmanr: 0.681654\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[28]\tvalid's rmse: 0.307607\tvalid's spearmanr: 0.656398\n28 answer_type_reason_explanation\n0.6711084706948391\n6079 746\n476 292\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[10]\tvalid's rmse: 0.0999711\tvalid's spearmanr: 0.201021\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[21]\tvalid's rmse: 0.0993344\tvalid's spearmanr: 0.187787\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[23]\tvalid's rmse: 0.0954789\tvalid's spearmanr: 0.200332\n29 answer_well_written\n0.19091961591525747\n6079 190\n476 177\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_spearmanr(trues, preds, n_bins=None):\n    rhos = []\n    if n_bins:\n        preds = binning_output(preds, n_bins)\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        if len(np.unique(col_pred)) == 1:\n            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n        rhos.append(spearmanr(col_trues, col_pred).correlation)\n    return np.mean(rhos)\n\n\noof_preds_fi = np.concatenate(oof_preds_list, axis=1)\nprint(compute_spearmanr(outputs, oof_preds_fi))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:14:53.181055Z","iopub.execute_input":"2024-05-04T21:14:53.181323Z","iopub.status.idle":"2024-05-04T21:14:53.252831Z","shell.execute_reply.started":"2024-05-04T21:14:53.181277Z","shell.execute_reply":"2024-05-04T21:14:53.252022Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"0.38275979422298306\n","output_type":"stream"}]},{"cell_type":"code","source":"test_preds_fi = np.concatenate(test_preds_list, axis=1)\nsub.iloc[:, 1:] = test_preds_fi\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:14:53.254063Z","iopub.execute_input":"2024-05-04T21:14:53.254318Z","iopub.status.idle":"2024-05-04T21:14:53.453886Z","shell.execute_reply.started":"2024-05-04T21:14:53.254276Z","shell.execute_reply":"2024-05-04T21:14:53.453270Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
