{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7968,
          "databundleVersionId": 828965,
          "sourceType": "competition"
        },
        {
          "sourceId": 256023,
          "sourceType": "datasetVersion",
          "datasetId": 107340
        }
      ],
      "dockerImageVersionId": 29850,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code is submitted by Vansh Maheshwari and Parth Parashar\n",
        "GROUP NO -8"
      ],
      "metadata": {
        "id": "2wAyEamUyswR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "import random\n",
        "import html\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold, KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import os\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.optimize import minimize\n",
        "from math import floor, ceil\n",
        "from transformers import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def tqdm(it, *args, **kwargs):\n",
        "    return it\n",
        "\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "seed_everything()\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "print(tf.__version__)\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:25:19.986583Z",
          "iopub.execute_input": "2024-04-20T15:25:19.986999Z",
          "iopub.status.idle": "2024-04-20T15:25:29.472683Z",
          "shell.execute_reply.started": "2024-04-20T15:25:19.986938Z",
          "shell.execute_reply": "2024-04-20T15:25:29.471795Z"
        },
        "trusted": true,
        "id": "5RpPUTk1mhf9",
        "outputId": "7c616c98-d1f9-4a89-e68a-8231f68a3d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "2.1.0\n1.3.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Read data and tokenizer"
      ],
      "metadata": {
        "id": "7yg55R9dmhgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '../input/google-quest-challenge/'\n",
        "\n",
        "BERT_PATH = '../input/bertpretrained/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "\n",
        "df_train = pd.read_csv(PATH+'train.csv')\n",
        "df_test = pd.read_csv(PATH+'test.csv')\n",
        "sub = pd.read_csv(PATH+'sample_submission.csv')\n",
        "print('train shape =', df_train.shape)\n",
        "print('test shape =', df_test.shape)\n",
        "\n",
        "output_categories = list(df_train.columns[11:])\n",
        "input_categories = list(df_train.columns[[1,2,5]])\n",
        "print('\\noutput categories:\\n\\t', output_categories)\n",
        "print('\\ninput categories:\\n\\t', input_categories)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:25:29.474840Z",
          "iopub.execute_input": "2024-04-20T15:25:29.475176Z",
          "iopub.status.idle": "2024-04-20T15:25:29.887948Z",
          "shell.execute_reply.started": "2024-04-20T15:25:29.475111Z",
          "shell.execute_reply": "2024-04-20T15:25:29.887181Z"
        },
        "trusted": true,
        "id": "_eBwd1aSmhgF",
        "outputId": "57f1dfd7-f842-4b10-99af-5e8a8dcd46aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "train shape = (6079, 41)\ntest shape = (476, 11)\n\noutput categories:\n\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n\ninput categories:\n\t ['question_title', 'question_body', 'answer']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing"
      ],
      "metadata": {
        "id": "Yf2ZKJ-3mhgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.question_body = df_train.question_body.apply(html.unescape)\n",
        "df_train.question_title = df_train.question_title.apply(html.unescape)\n",
        "df_train.answer = df_train.answer.apply(html.unescape)\n",
        "\n",
        "df_test.question_body = df_test.question_body.apply(html.unescape)\n",
        "df_test.question_title = df_test.question_title.apply(html.unescape)\n",
        "df_test.answer = df_test.answer.apply(html.unescape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:25:29.891226Z",
          "iopub.execute_input": "2024-04-20T15:25:29.891553Z",
          "iopub.status.idle": "2024-04-20T15:25:29.961071Z",
          "shell.execute_reply.started": "2024-04-20T15:25:29.891501Z",
          "shell.execute_reply": "2024-04-20T15:25:29.960495Z"
        },
        "trusted": true,
        "id": "XjLv3NjLmhgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _preprocess_text(s: str) -> str:\n",
        "    return s\n",
        "\n",
        "\n",
        "def _trim_input(question_tokens: List[str], answer_tokens: List[str], max_sequence_length: int, q_max_len: int, a_max_len: int) -> Tuple[List[str], List[str]]:\n",
        "    q_len = len(question_tokens)\n",
        "    a_len = len(answer_tokens)\n",
        "    if q_len + a_len + 3 > max_sequence_length:\n",
        "        if a_max_len <= a_len and q_max_len <= q_len:\n",
        "            ## Answer も Question も長過ぎる場合、どちらも限界まで切り詰めるしかない\n",
        "            q_new_len_head = floor((q_max_len - q_max_len/2))\n",
        "            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n",
        "            a_new_len_head = floor((a_max_len - a_max_len/2))\n",
        "            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n",
        "        elif q_len <= a_len and q_len < q_max_len:\n",
        "            ## Answer のほうが長く、Question が十分短いなら、その分 Answer にまわす\n",
        "            a_max_len = a_max_len + (q_max_len - q_len - 1)\n",
        "            a_new_len_head = floor((a_max_len - a_max_len/2))\n",
        "            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n",
        "        elif a_len < q_len:\n",
        "            assert a_len <= a_max_len\n",
        "            q_max_len = q_max_len + (a_max_len - a_len - 1)\n",
        "            q_new_len_head = floor((q_max_len - q_max_len/2))\n",
        "            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n",
        "        else:\n",
        "            raise ValueError(\"unreachable: q_len: {}, a_len: {}, q_max_len: {}, a_max_len: {}\".format(q_len, a_len, q_max_len, a_max_len))\n",
        "    return question_tokens, answer_tokens\n",
        "\n",
        "\n",
        "def _convert_to_transformer_inputs(title: str, question: str, answer: str, tokenizer: BertTokenizer, question_only=False):\n",
        "    title = _preprocess_text(title)\n",
        "    question = _preprocess_text(question)\n",
        "    answer = _preprocess_text(answer)\n",
        "    question = \"{} [SEP] {}\".format(title, question)\n",
        "    question_tokens = tokenizer.tokenize(question)\n",
        "    if question_only:\n",
        "        answer_tokens = []\n",
        "    else:\n",
        "        answer_tokens = tokenizer.tokenize(answer)\n",
        "    question_tokens, answer_tokens = _trim_input(question_tokens, answer_tokens, MAX_SEQUENCE_LENGTH, (MAX_SEQUENCE_LENGTH - 3) // 2, (MAX_SEQUENCE_LENGTH - 3) // 2)\n",
        "    ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + question_tokens + [\"[SEP]\"] + answer_tokens + [\"[SEP]\"])\n",
        "    padded_ids = ids + [tokenizer.pad_token_id] * (MAX_SEQUENCE_LENGTH - len(ids))\n",
        "    token_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(answer_tokens) + 1) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n",
        "    attention_mask = [1] * len(ids) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n",
        "    return padded_ids, token_type_ids, attention_mask\n",
        "\n",
        "sample_args = df_train[\"question_title\"].values[0], df_train[\"question_body\"].values[0], df_train[\"answer\"].values[0]\n",
        "sample_ids = _convert_to_transformer_inputs(*sample_args, tokenizer, question_only=True)\n",
        "print(sample_ids)\n",
        "print(tokenizer.convert_ids_to_tokens(sample_ids[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:25:29.962728Z",
          "iopub.execute_input": "2024-04-20T15:25:29.963148Z",
          "iopub.status.idle": "2024-04-20T15:25:29.996680Z",
          "shell.execute_reply.started": "2024-04-20T15:25:29.963003Z",
          "shell.execute_reply": "2024-04-20T15:25:29.995836Z"
        },
        "trusted": true,
        "id": "frHA5cIomhgH",
        "outputId": "7ea594ce-64c1-4a01-be22-725c23818d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "([101, 2054, 2572, 1045, 3974, 2043, 2478, 5331, 10868, 2612, 1997, 1037, 26632, 10014, 1029, 102, 2044, 2652, 2105, 2007, 26632, 5855, 2006, 1011, 1996, 1011, 10036, 1006, 3191, 1024, 11674, 10014, 1010, 7065, 1012, 10014, 5614, 2006, 1037, 3442, 10014, 1010, 13135, 5331, 10868, 1007, 1010, 1045, 2052, 2066, 2000, 2131, 2582, 2007, 2023, 1012, 1996, 3471, 2007, 1996, 5461, 1045, 2109, 2003, 2008, 3579, 2003, 6410, 1998, 18892, 2491, 2003, 18636, 2012, 2190, 1012, 2023, 3132, 2026, 16437, 2000, 2145, 5739, 1006, 3191, 1024, 2757, 9728, 1007, 2085, 1010, 2004, 3500, 2003, 8455, 1010, 1045, 2215, 2000, 2022, 2583, 2000, 5607, 2444, 9728, 1012, 1045, 2903, 2008, 2005, 2023, 1010, 8285, 14876, 7874, 1998, 2275, 10880, 18892, 2097, 2022, 1997, 2307, 2393, 1012, 2061, 1010, 2028, 5793, 2021, 6450, 5724, 2003, 1037, 26632, 10014, 1006, 2360, 1010, 1041, 2546, 2531, 7382, 26632, 1007, 2174, 1010, 1045, 2572, 2025, 2428, 4699, 1999, 2664, 2178, 3539, 10014, 1012, 2019, 4522, 2003, 1996, 5992, 5331, 10868, 1012, 3272, 2005, 4555, 7995, 3292, 1010, 2054, 2572, 1045, 3974, 2043, 2478, 10868, 1006, 11211, 2007, 1037, 2986, 10014, 1010, 2360, 1041, 2546, 19841, 1011, 3263, 1013, 1016, 1012, 1022, 1007, 2612, 1997, 1037, 26632, 10014, 1029, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n['[CLS]', 'what', 'am', 'i', 'losing', 'when', 'using', 'extension', 'tubes', 'instead', 'of', 'a', 'macro', 'lens', '?', '[SEP]', 'after', 'playing', 'around', 'with', 'macro', 'photography', 'on', '-', 'the', '-', 'cheap', '(', 'read', ':', 'reversed', 'lens', ',', 'rev', '.', 'lens', 'mounted', 'on', 'a', 'straight', 'lens', ',', 'passive', 'extension', 'tubes', ')', ',', 'i', 'would', 'like', 'to', 'get', 'further', 'with', 'this', '.', 'the', 'problems', 'with', 'the', 'techniques', 'i', 'used', 'is', 'that', 'focus', 'is', 'manual', 'and', 'aperture', 'control', 'is', 'problematic', 'at', 'best', '.', 'this', 'limited', 'my', 'setup', 'to', 'still', 'subjects', '(', 'read', ':', 'dead', 'insects', ')', 'now', ',', 'as', 'spring', 'is', 'approaching', ',', 'i', 'want', 'to', 'be', 'able', 'to', 'shoot', 'live', 'insects', '.', 'i', 'believe', 'that', 'for', 'this', ',', 'auto', '##fo', '##cus', 'and', 'set', '##table', 'aperture', 'will', 'be', 'of', 'great', 'help', '.', 'so', ',', 'one', 'obvious', 'but', 'expensive', 'option', 'is', 'a', 'macro', 'lens', '(', 'say', ',', 'e', '##f', '100', '##mm', 'macro', ')', 'however', ',', 'i', 'am', 'not', 'really', 'interested', 'in', 'yet', 'another', 'prime', 'lens', '.', 'an', 'alternative', 'is', 'the', 'electrical', 'extension', 'tubes', '.', 'except', 'for', 'maximum', 'focusing', 'distance', ',', 'what', 'am', 'i', 'losing', 'when', 'using', 'tubes', '(', 'coupled', 'with', 'a', 'fine', 'lens', ',', 'say', 'e', '##f', '##70', '-', '200', '/', '2', '.', '8', ')', 'instead', 'of', 'a', 'macro', 'lens', '?', '[SEP]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_input_arrays(df, question_only=False):\n",
        "    input_ids, input_token_type_ids, input_attention_masks = [], [], []\n",
        "    for title, body, answer in zip(df[\"question_title\"].values, df[\"question_body\"].values, df[\"answer\"].values):\n",
        "        ids, type_ids, mask = _convert_to_transformer_inputs(title, body, answer, tokenizer, question_only=question_only)\n",
        "        input_ids.append(ids)\n",
        "        input_token_type_ids.append(type_ids)\n",
        "        input_attention_masks.append(mask)\n",
        "    return (\n",
        "        np.asarray(input_ids, dtype=np.int32),\n",
        "        np.asarray(input_token_type_ids, dtype=np.int32),\n",
        "        np.asarray(input_attention_masks, dtype=np.int32),\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_output_arrays(df):\n",
        "    return np.asarray(df[output_categories])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:25:29.999971Z",
          "iopub.execute_input": "2024-04-20T15:25:30.000254Z",
          "iopub.status.idle": "2024-04-20T15:25:30.011894Z",
          "shell.execute_reply.started": "2024-04-20T15:25:30.000178Z",
          "shell.execute_reply": "2024-04-20T15:25:30.011009Z"
        },
        "trusted": true,
        "id": "ho7-Udz2mhgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modeling"
      ],
      "metadata": {
        "id": "raKRxr0fmhgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        config = BertConfig.from_json_file(BERT_PATH + \"/bert_config.json\")\n",
        "        config.output_hidden_states = True\n",
        "        self.bert = BertForPreTraining.from_pretrained(BERT_PATH + \"/bert_model.ckpt.index\", from_tf=True, config=config).bert\n",
        "        self.cls_token_head = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768 * 4, 768),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.qa_sep_token_head = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768 * 4, 768),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(768 * 2, 30),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        question_answer_seps = (torch.sum((token_type_ids == 0) * attention_mask, -1) - 1)\n",
        "\n",
        "#         p_question_answer_dropout = 0.2\n",
        "#         if self.training and random.random() < p_question_answer_dropout:\n",
        "#             if random.random() < 0.5:\n",
        "#                 # mask question\n",
        "#                 attention_mask = attention_mask * (token_type_ids == 1)\n",
        "#             else:\n",
        "#                 # mask answer\n",
        "#                 attention_mask = attention_mask * (token_type_ids == 0)\n",
        "\n",
        "        _, _, hidden_states = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_states_cls_embeddings = [x[:, 0] for x in hidden_states[-4:]]\n",
        "        x = torch.cat(hidden_states_cls_embeddings, dim=-1)\n",
        "        x_cls = self.cls_token_head(x)\n",
        "\n",
        "        # Gather [SEP] hidden states\n",
        "        tmp = torch.arange(0, len(input_ids), dtype=torch.long)\n",
        "        hidden_states_qa_sep_embeddings = [x[tmp, question_answer_seps] for x in hidden_states[-4:]]\n",
        "        x = torch.cat(hidden_states_qa_sep_embeddings, dim=-1)\n",
        "\n",
        "        x_qa_sep = self.qa_sep_token_head(x)\n",
        "        x = torch.cat([x_cls, x_qa_sep], -1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:25:30.014535Z",
          "iopub.execute_input": "2024-04-20T15:25:30.014890Z",
          "iopub.status.idle": "2024-04-20T15:25:30.031821Z",
          "shell.execute_reply.started": "2024-04-20T15:25:30.014796Z",
          "shell.execute_reply": "2024-04-20T15:25:30.030948Z"
        },
        "trusted": true,
        "id": "FCwojTCPmhgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training"
      ],
      "metadata": {
        "id": "Mtu0XLtwmhgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = torch.tensor(compute_output_arrays(df_train), dtype=torch.float)\n",
        "inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train)]\n",
        "question_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train, question_only=True)]\n",
        "test_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test)]\n",
        "test_question_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test, question_only=True)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:25:30.033008Z",
          "iopub.execute_input": "2024-04-20T15:25:30.033267Z",
          "iopub.status.idle": "2024-04-20T15:27:22.589480Z",
          "shell.execute_reply.started": "2024-04-20T15:25:30.033213Z",
          "shell.execute_reply": "2024-04-20T15:27:22.588647Z"
        },
        "trusted": true,
        "id": "h4hldC6RmhgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:27:22.590929Z",
          "iopub.execute_input": "2024-04-20T15:27:22.591413Z",
          "iopub.status.idle": "2024-04-20T15:27:22.648850Z",
          "shell.execute_reply.started": "2024-04-20T15:27:22.591221Z",
          "shell.execute_reply": "2024-04-20T15:27:22.648132Z"
        },
        "trusted": true,
        "id": "9-DH3lVdmhgW",
        "outputId": "18b970d2-3687-427f-eb60-7d40628c8a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "cuda\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n, _ in Model().named_parameters():\n",
        "    print(n)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:27:22.650184Z",
          "iopub.execute_input": "2024-04-20T15:27:22.650519Z",
          "iopub.status.idle": "2024-04-20T15:27:29.816931Z",
          "shell.execute_reply.started": "2024-04-20T15:27:22.650472Z",
          "shell.execute_reply": "2024-04-20T15:27:29.816182Z"
        },
        "trusted": true,
        "id": "FqOZukXWmhgX",
        "outputId": "9792dfab-9719-4307-de90-8127c4135984"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "bert.embeddings.word_embeddings.weight\nbert.embeddings.position_embeddings.weight\nbert.embeddings.token_type_embeddings.weight\nbert.embeddings.LayerNorm.weight\nbert.embeddings.LayerNorm.bias\nbert.encoder.layer.0.attention.self.query.weight\nbert.encoder.layer.0.attention.self.query.bias\nbert.encoder.layer.0.attention.self.key.weight\nbert.encoder.layer.0.attention.self.key.bias\nbert.encoder.layer.0.attention.self.value.weight\nbert.encoder.layer.0.attention.self.value.bias\nbert.encoder.layer.0.attention.output.dense.weight\nbert.encoder.layer.0.attention.output.dense.bias\nbert.encoder.layer.0.attention.output.LayerNorm.weight\nbert.encoder.layer.0.attention.output.LayerNorm.bias\nbert.encoder.layer.0.intermediate.dense.weight\nbert.encoder.layer.0.intermediate.dense.bias\nbert.encoder.layer.0.output.dense.weight\nbert.encoder.layer.0.output.dense.bias\nbert.encoder.layer.0.output.LayerNorm.weight\nbert.encoder.layer.0.output.LayerNorm.bias\nbert.encoder.layer.1.attention.self.query.weight\nbert.encoder.layer.1.attention.self.query.bias\nbert.encoder.layer.1.attention.self.key.weight\nbert.encoder.layer.1.attention.self.key.bias\nbert.encoder.layer.1.attention.self.value.weight\nbert.encoder.layer.1.attention.self.value.bias\nbert.encoder.layer.1.attention.output.dense.weight\nbert.encoder.layer.1.attention.output.dense.bias\nbert.encoder.layer.1.attention.output.LayerNorm.weight\nbert.encoder.layer.1.attention.output.LayerNorm.bias\nbert.encoder.layer.1.intermediate.dense.weight\nbert.encoder.layer.1.intermediate.dense.bias\nbert.encoder.layer.1.output.dense.weight\nbert.encoder.layer.1.output.dense.bias\nbert.encoder.layer.1.output.LayerNorm.weight\nbert.encoder.layer.1.output.LayerNorm.bias\nbert.encoder.layer.2.attention.self.query.weight\nbert.encoder.layer.2.attention.self.query.bias\nbert.encoder.layer.2.attention.self.key.weight\nbert.encoder.layer.2.attention.self.key.bias\nbert.encoder.layer.2.attention.self.value.weight\nbert.encoder.layer.2.attention.self.value.bias\nbert.encoder.layer.2.attention.output.dense.weight\nbert.encoder.layer.2.attention.output.dense.bias\nbert.encoder.layer.2.attention.output.LayerNorm.weight\nbert.encoder.layer.2.attention.output.LayerNorm.bias\nbert.encoder.layer.2.intermediate.dense.weight\nbert.encoder.layer.2.intermediate.dense.bias\nbert.encoder.layer.2.output.dense.weight\nbert.encoder.layer.2.output.dense.bias\nbert.encoder.layer.2.output.LayerNorm.weight\nbert.encoder.layer.2.output.LayerNorm.bias\nbert.encoder.layer.3.attention.self.query.weight\nbert.encoder.layer.3.attention.self.query.bias\nbert.encoder.layer.3.attention.self.key.weight\nbert.encoder.layer.3.attention.self.key.bias\nbert.encoder.layer.3.attention.self.value.weight\nbert.encoder.layer.3.attention.self.value.bias\nbert.encoder.layer.3.attention.output.dense.weight\nbert.encoder.layer.3.attention.output.dense.bias\nbert.encoder.layer.3.attention.output.LayerNorm.weight\nbert.encoder.layer.3.attention.output.LayerNorm.bias\nbert.encoder.layer.3.intermediate.dense.weight\nbert.encoder.layer.3.intermediate.dense.bias\nbert.encoder.layer.3.output.dense.weight\nbert.encoder.layer.3.output.dense.bias\nbert.encoder.layer.3.output.LayerNorm.weight\nbert.encoder.layer.3.output.LayerNorm.bias\nbert.encoder.layer.4.attention.self.query.weight\nbert.encoder.layer.4.attention.self.query.bias\nbert.encoder.layer.4.attention.self.key.weight\nbert.encoder.layer.4.attention.self.key.bias\nbert.encoder.layer.4.attention.self.value.weight\nbert.encoder.layer.4.attention.self.value.bias\nbert.encoder.layer.4.attention.output.dense.weight\nbert.encoder.layer.4.attention.output.dense.bias\nbert.encoder.layer.4.attention.output.LayerNorm.weight\nbert.encoder.layer.4.attention.output.LayerNorm.bias\nbert.encoder.layer.4.intermediate.dense.weight\nbert.encoder.layer.4.intermediate.dense.bias\nbert.encoder.layer.4.output.dense.weight\nbert.encoder.layer.4.output.dense.bias\nbert.encoder.layer.4.output.LayerNorm.weight\nbert.encoder.layer.4.output.LayerNorm.bias\nbert.encoder.layer.5.attention.self.query.weight\nbert.encoder.layer.5.attention.self.query.bias\nbert.encoder.layer.5.attention.self.key.weight\nbert.encoder.layer.5.attention.self.key.bias\nbert.encoder.layer.5.attention.self.value.weight\nbert.encoder.layer.5.attention.self.value.bias\nbert.encoder.layer.5.attention.output.dense.weight\nbert.encoder.layer.5.attention.output.dense.bias\nbert.encoder.layer.5.attention.output.LayerNorm.weight\nbert.encoder.layer.5.attention.output.LayerNorm.bias\nbert.encoder.layer.5.intermediate.dense.weight\nbert.encoder.layer.5.intermediate.dense.bias\nbert.encoder.layer.5.output.dense.weight\nbert.encoder.layer.5.output.dense.bias\nbert.encoder.layer.5.output.LayerNorm.weight\nbert.encoder.layer.5.output.LayerNorm.bias\nbert.encoder.layer.6.attention.self.query.weight\nbert.encoder.layer.6.attention.self.query.bias\nbert.encoder.layer.6.attention.self.key.weight\nbert.encoder.layer.6.attention.self.key.bias\nbert.encoder.layer.6.attention.self.value.weight\nbert.encoder.layer.6.attention.self.value.bias\nbert.encoder.layer.6.attention.output.dense.weight\nbert.encoder.layer.6.attention.output.dense.bias\nbert.encoder.layer.6.attention.output.LayerNorm.weight\nbert.encoder.layer.6.attention.output.LayerNorm.bias\nbert.encoder.layer.6.intermediate.dense.weight\nbert.encoder.layer.6.intermediate.dense.bias\nbert.encoder.layer.6.output.dense.weight\nbert.encoder.layer.6.output.dense.bias\nbert.encoder.layer.6.output.LayerNorm.weight\nbert.encoder.layer.6.output.LayerNorm.bias\nbert.encoder.layer.7.attention.self.query.weight\nbert.encoder.layer.7.attention.self.query.bias\nbert.encoder.layer.7.attention.self.key.weight\nbert.encoder.layer.7.attention.self.key.bias\nbert.encoder.layer.7.attention.self.value.weight\nbert.encoder.layer.7.attention.self.value.bias\nbert.encoder.layer.7.attention.output.dense.weight\nbert.encoder.layer.7.attention.output.dense.bias\nbert.encoder.layer.7.attention.output.LayerNorm.weight\nbert.encoder.layer.7.attention.output.LayerNorm.bias\nbert.encoder.layer.7.intermediate.dense.weight\nbert.encoder.layer.7.intermediate.dense.bias\nbert.encoder.layer.7.output.dense.weight\nbert.encoder.layer.7.output.dense.bias\nbert.encoder.layer.7.output.LayerNorm.weight\nbert.encoder.layer.7.output.LayerNorm.bias\nbert.encoder.layer.8.attention.self.query.weight\nbert.encoder.layer.8.attention.self.query.bias\nbert.encoder.layer.8.attention.self.key.weight\nbert.encoder.layer.8.attention.self.key.bias\nbert.encoder.layer.8.attention.self.value.weight\nbert.encoder.layer.8.attention.self.value.bias\nbert.encoder.layer.8.attention.output.dense.weight\nbert.encoder.layer.8.attention.output.dense.bias\nbert.encoder.layer.8.attention.output.LayerNorm.weight\nbert.encoder.layer.8.attention.output.LayerNorm.bias\nbert.encoder.layer.8.intermediate.dense.weight\nbert.encoder.layer.8.intermediate.dense.bias\nbert.encoder.layer.8.output.dense.weight\nbert.encoder.layer.8.output.dense.bias\nbert.encoder.layer.8.output.LayerNorm.weight\nbert.encoder.layer.8.output.LayerNorm.bias\nbert.encoder.layer.9.attention.self.query.weight\nbert.encoder.layer.9.attention.self.query.bias\nbert.encoder.layer.9.attention.self.key.weight\nbert.encoder.layer.9.attention.self.key.bias\nbert.encoder.layer.9.attention.self.value.weight\nbert.encoder.layer.9.attention.self.value.bias\nbert.encoder.layer.9.attention.output.dense.weight\nbert.encoder.layer.9.attention.output.dense.bias\nbert.encoder.layer.9.attention.output.LayerNorm.weight\nbert.encoder.layer.9.attention.output.LayerNorm.bias\nbert.encoder.layer.9.intermediate.dense.weight\nbert.encoder.layer.9.intermediate.dense.bias\nbert.encoder.layer.9.output.dense.weight\nbert.encoder.layer.9.output.dense.bias\nbert.encoder.layer.9.output.LayerNorm.weight\nbert.encoder.layer.9.output.LayerNorm.bias\nbert.encoder.layer.10.attention.self.query.weight\nbert.encoder.layer.10.attention.self.query.bias\nbert.encoder.layer.10.attention.self.key.weight\nbert.encoder.layer.10.attention.self.key.bias\nbert.encoder.layer.10.attention.self.value.weight\nbert.encoder.layer.10.attention.self.value.bias\nbert.encoder.layer.10.attention.output.dense.weight\nbert.encoder.layer.10.attention.output.dense.bias\nbert.encoder.layer.10.attention.output.LayerNorm.weight\nbert.encoder.layer.10.attention.output.LayerNorm.bias\nbert.encoder.layer.10.intermediate.dense.weight\nbert.encoder.layer.10.intermediate.dense.bias\nbert.encoder.layer.10.output.dense.weight\nbert.encoder.layer.10.output.dense.bias\nbert.encoder.layer.10.output.LayerNorm.weight\nbert.encoder.layer.10.output.LayerNorm.bias\nbert.encoder.layer.11.attention.self.query.weight\nbert.encoder.layer.11.attention.self.query.bias\nbert.encoder.layer.11.attention.self.key.weight\nbert.encoder.layer.11.attention.self.key.bias\nbert.encoder.layer.11.attention.self.value.weight\nbert.encoder.layer.11.attention.self.value.bias\nbert.encoder.layer.11.attention.output.dense.weight\nbert.encoder.layer.11.attention.output.dense.bias\nbert.encoder.layer.11.attention.output.LayerNorm.weight\nbert.encoder.layer.11.attention.output.LayerNorm.bias\nbert.encoder.layer.11.intermediate.dense.weight\nbert.encoder.layer.11.intermediate.dense.bias\nbert.encoder.layer.11.output.dense.weight\nbert.encoder.layer.11.output.dense.bias\nbert.encoder.layer.11.output.LayerNorm.weight\nbert.encoder.layer.11.output.LayerNorm.bias\nbert.pooler.dense.weight\nbert.pooler.dense.bias\ncls_token_head.1.weight\ncls_token_head.1.bias\nqa_sep_token_head.1.weight\nqa_sep_token_head.1.bias\nlinear.1.weight\nlinear.1.bias\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_WEIGHTS = torch.tensor(1.0 / df_train[output_categories].std().values, dtype=torch.float32).to(device)\n",
        "LABEL_WEIGHTS = LABEL_WEIGHTS / LABEL_WEIGHTS.sum() * 30\n",
        "for name, weight in zip(output_categories, LABEL_WEIGHTS.cpu().numpy()):\n",
        "    print(name, \"\\t\", weight)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:27:29.818117Z",
          "iopub.execute_input": "2024-04-20T15:27:29.818426Z",
          "iopub.status.idle": "2024-04-20T15:27:36.835617Z",
          "shell.execute_reply.started": "2024-04-20T15:27:29.818376Z",
          "shell.execute_reply": "2024-04-20T15:27:36.834801Z"
        },
        "trusted": true,
        "id": "9OuCZDrKmhgX",
        "outputId": "ef25b3f1-a5f7-46f8-9061-115e12fa0453"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "question_asker_intent_understanding \t 0.9666648\nquestion_body_critical \t 0.58160704\nquestion_conversational \t 0.7005903\nquestion_expect_short_answer \t 0.36372498\nquestion_fact_seeking \t 0.4212381\nquestion_has_commonly_accepted_answer \t 0.3791943\nquestion_interestingness_others \t 0.9392594\nquestion_interestingness_self \t 0.6863097\nquestion_multi_intent \t 0.3809655\nquestion_not_really_a_question \t 2.7880754\nquestion_opinion_seeking \t 0.34880248\nquestion_type_choice \t 0.346085\nquestion_type_compare \t 0.8308305\nquestion_type_consequence \t 1.7193657\nquestion_type_definition \t 0.92452765\nquestion_type_entity \t 0.646035\nquestion_type_instructions \t 0.3016625\nquestion_type_procedure \t 0.4960922\nquestion_type_reason_explanation \t 0.33294326\nquestion_type_spelling \t 6.230046\nquestion_well_written \t 0.7154175\nanswer_helpful \t 1.1115448\nanswer_level_of_information \t 1.1855657\nanswer_plausible \t 1.4684314\nanswer_relevance \t 1.7103598\nanswer_satisfaction \t 0.97630584\nanswer_type_instructions \t 0.3018177\nanswer_type_procedure \t 0.56550634\nanswer_type_reason_explanation \t 0.3135491\nanswer_well_written \t 1.2674823\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BEST_BINS = [400, 400, 15, 100, 400, 7, 1600, 100, 100, 400, 100, 9, 8, 50, 9, 8, 15, 400, 400, 5, 400, 400, 800, 50, 200, 1600, 20, 200, 1600, 1600]\n",
        "\n",
        "def binning_output(preds, n_bins=BEST_BINS):\n",
        "    preds = preds.copy()\n",
        "    for i in range(preds.shape[-1]):\n",
        "        n = n_bins[i]\n",
        "        binned = (preds[:, i] * n).astype(np.int32).astype(np.float32) / n\n",
        "        unique_values, unique_counts = np.unique(binned, return_counts=True)\n",
        "        # 多数派以外が 0.5 % を下回ったら binning をやめる\n",
        "        minor_value_ratio = (unique_counts.sum() - unique_counts.max()) / unique_counts.sum()\n",
        "        if minor_value_ratio < 0.005:\n",
        "            keep = np.argsort(preds[:, i])[::-1][:int(len(preds) * 0.005) + 1]\n",
        "            binned[keep] = preds[keep, i]\n",
        "        preds[:, i] = binned\n",
        "    return preds\n",
        "\n",
        "\n",
        "def compute_spearmanr(trues, preds, n_bins=None):\n",
        "    rhos = []\n",
        "    if n_bins:\n",
        "        preds = binning_output(preds, n_bins)\n",
        "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
        "        if len(np.unique(col_pred)) == 1:\n",
        "            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n",
        "        rhos.append(spearmanr(col_trues, col_pred).correlation)\n",
        "    return np.mean(rhos)\n",
        "\n",
        "\n",
        "\n",
        "def compute_loss(outputs, targets, alpha=0.5, margin=0.1, question_only=False):\n",
        "    if question_only:\n",
        "        outputs = outputs[:, :21]\n",
        "        targets = targets[:, :21]\n",
        "    bce = F.binary_cross_entropy_with_logits(outputs, targets, reduction=\"none\")\n",
        "    bce = (bce * LABEL_WEIGHTS[:bce.size(-1)]).mean()\n",
        "\n",
        "    batch_size = outputs.size(0)\n",
        "    if batch_size % 2 == 0:\n",
        "        outputs1, outputs2 = outputs.sigmoid().contiguous().view(2, batch_size // 2, outputs.size(-1))\n",
        "        targets1, targets2 = targets.contiguous().view(2, batch_size // 2, outputs.size(-1))\n",
        "        # 1 if first ones are larger, -1 if second ones are larger, and 0 if equals.\n",
        "        ordering = (targets1 > targets2).float() - (targets1 < targets2).float()\n",
        "        margin_rank_loss = (-ordering * (outputs1 - outputs2) + margin).clamp(min=0.0)\n",
        "        margin_rank_loss = (margin_rank_loss * LABEL_WEIGHTS[:outputs.size(-1)]).mean()\n",
        "    else:\n",
        "        # batch size is not even number, so we can't devide them into pairs.\n",
        "        margin_rank_loss = 0.0\n",
        "\n",
        "    return alpha * bce + (1 - alpha) * margin_rank_loss\n",
        "\n",
        "\n",
        "def train_and_predict(train_data, valid_data, test_data, q_train_data, q_valid_data, q_test_data, q_epochs, epochs, batch_size, fold):\n",
        "    dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
        "    q_dataloader = torch.utils.data.DataLoader(q_train_data, shuffle=True, batch_size=batch_size)\n",
        "    q_valid_dataloader = torch.utils.data.DataLoader(q_valid_data, shuffle=False, batch_size=batch_size)\n",
        "    q_test_dataloader = torch.utils.data.DataLoader(q_test_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    model = Model().to(device)\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n",
        "            \"weight_decay\": 1e-2,\n",
        "            \"lr\": 5e-5\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": 5e-5\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n",
        "            \"weight_decay\": 1e-2,\n",
        "            \"lr\": 5e-4\n",
        "\n",
        "        }\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=int(len(dataloader) * (q_epochs) * 0.05),\n",
        "        num_training_steps=len(dataloader) * (q_epochs)\n",
        "    )\n",
        "\n",
        "    test_predictions = []\n",
        "    valid_predictions = []\n",
        "\n",
        "    ## Question Only\n",
        "    for epoch in range(q_epochs):\n",
        "        import time\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_preds = []\n",
        "        train_targets = []\n",
        "        for input_ids, token_type_ids, attention_mask, targets in tqdm(q_dataloader, total=len(q_dataloader)):\n",
        "            input_ids = input_ids.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n",
        "            train_targets.extend(targets.detach().cpu().numpy())\n",
        "            loss = compute_loss(outputs, targets, question_only=True)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_losses.append(loss.detach().cpu().item())\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        valid_preds = []\n",
        "        valid_targets = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, token_type_ids, attention_mask, targets in tqdm(q_valid_dataloader, total=len(q_valid_dataloader)):\n",
        "                input_ids = input_ids.to(device)\n",
        "                token_type_ids = token_type_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "                prob = outputs.sigmoid()\n",
        "                prob[:, 21:] = 0.0\n",
        "                valid_preds.extend(prob.cpu().numpy())\n",
        "                valid_targets.extend(targets.cpu().numpy())\n",
        "                loss = compute_loss(outputs, targets, question_only=True)\n",
        "                valid_losses.append(loss.detach().cpu().item())\n",
        "            valid_predictions.append(np.stack(valid_preds))\n",
        "            test_preds = []\n",
        "            for input_ids, token_type_ids, attention_mask in tqdm(q_test_dataloader, total=len(q_test_dataloader)):\n",
        "                input_ids = input_ids.to(device)\n",
        "                token_type_ids = token_type_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "                prob = outputs.sigmoid()\n",
        "                prob[:, 21:] = 0.0\n",
        "                test_preds.extend(prob.cpu().numpy())\n",
        "            test_predictions.append(np.stack(test_preds))\n",
        "            print()\n",
        "        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n",
        "        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n",
        "            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n",
        "            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) / len(valid_predictions)),\n",
        "            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n",
        "        ))\n",
        "        print(\"\\t elapsed: {}s\".format(time.time() - start))\n",
        "\n",
        "    ## Q and A\n",
        "    model = Model().to(device)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n",
        "            \"weight_decay\": 1e-2,\n",
        "            \"lr\": 5e-5\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": 5e-5\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n",
        "            \"weight_decay\": 1e-2,\n",
        "            \"lr\": 5e-4\n",
        "\n",
        "        }\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=int(len(dataloader) * (epochs) * 0.05),\n",
        "        num_training_steps=len(dataloader) * (epochs)\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        import time\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_preds = []\n",
        "        train_targets = []\n",
        "        for input_ids, token_type_ids, attention_mask, targets in tqdm(dataloader, total=len(dataloader)):\n",
        "            input_ids = input_ids.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n",
        "            train_targets.extend(targets.detach().cpu().numpy())\n",
        "            loss = compute_loss(outputs, targets)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_losses.append(loss.detach().cpu().item())\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        valid_preds = []\n",
        "        valid_targets = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, token_type_ids, attention_mask, targets in tqdm(valid_dataloader, total=len(valid_dataloader)):\n",
        "                input_ids = input_ids.to(device)\n",
        "                token_type_ids = token_type_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "                valid_preds.extend(outputs.sigmoid().cpu().numpy())\n",
        "                valid_targets.extend(targets.cpu().numpy())\n",
        "                loss = compute_loss(outputs, targets)\n",
        "                valid_losses.append(loss.detach().cpu().item())\n",
        "            valid_predictions.append(np.stack(valid_preds))\n",
        "            test_preds = []\n",
        "            for input_ids, token_type_ids, attention_mask in tqdm(test_dataloader, total=len(test_dataloader)):\n",
        "                input_ids = input_ids.to(device)\n",
        "                token_type_ids = token_type_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "                test_preds.extend(outputs.sigmoid().cpu().numpy())\n",
        "            test_predictions.append(np.stack(test_preds))\n",
        "            print()\n",
        "        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n",
        "        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n",
        "            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n",
        "            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) / len(valid_predictions)),\n",
        "            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n",
        "        ))\n",
        "        print(\"\\t elapsed: {}s\".format(time.time() - start))\n",
        "\n",
        "    return valid_predictions, test_predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:27:36.837534Z",
          "iopub.execute_input": "2024-04-20T15:27:36.837873Z",
          "iopub.status.idle": "2024-04-20T15:27:36.924371Z",
          "shell.execute_reply.started": "2024-04-20T15:27:36.837813Z",
          "shell.execute_reply": "2024-04-20T15:27:36.923548Z"
        },
        "trusted": true,
        "id": "OmO_itF6mhgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Fold(object):\n",
        "    def __init__(self, n_splits=5, shuffle=True, random_state=71):\n",
        "        self.n_splits = n_splits\n",
        "        self.shuffle = shuffle\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def get_groupkfold(self, train, group_name):\n",
        "        group = train[group_name]\n",
        "        unique_group = group.unique()\n",
        "\n",
        "        kf = KFold(\n",
        "            n_splits=self.n_splits,\n",
        "            shuffle=self.shuffle,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        folds_ids = []\n",
        "        for trn_group_idx, val_group_idx in kf.split(unique_group):\n",
        "            trn_group = unique_group[trn_group_idx]\n",
        "            val_group = unique_group[val_group_idx]\n",
        "            is_trn = group.isin(trn_group)\n",
        "            is_val = group.isin(val_group)\n",
        "            trn_idx = train[is_trn].index\n",
        "            val_idx = train[is_val].index\n",
        "            folds_ids.append((trn_idx, val_idx))\n",
        "\n",
        "        return folds_ids"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:27:36.925732Z",
          "iopub.execute_input": "2024-04-20T15:27:36.926114Z",
          "iopub.status.idle": "2024-04-20T15:27:36.943075Z",
          "shell.execute_reply.started": "2024-04-20T15:27:36.926037Z",
          "shell.execute_reply": "2024-04-20T15:27:36.942305Z"
        },
        "trusted": true,
        "id": "k5pDCfvVmhgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gkf = Fold(n_splits=3, shuffle=True, random_state=71)\n",
        "fold_ids = gkf.get_groupkfold(df_train, group_name=\"url\")\n",
        "\n",
        "for train_idx, valid_idx in fold_ids:\n",
        "    print((df_train.loc[train_idx, \"question_type_spelling\"] > 0).sum())\n",
        "    print((df_train.loc[valid_idx, \"question_type_spelling\"] > 0).sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:27:36.944386Z",
          "iopub.execute_input": "2024-04-20T15:27:36.944637Z",
          "iopub.status.idle": "2024-04-20T15:27:36.985341Z",
          "shell.execute_reply.started": "2024-04-20T15:27:36.944592Z",
          "shell.execute_reply": "2024-04-20T15:27:36.984612Z"
        },
        "trusted": true,
        "id": "Y6K334bqmhgZ",
        "outputId": "9571f246-4e77-481a-a36d-7c211664b83a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "6\n5\n6\n5\n10\n1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "histories = []\n",
        "test_dataset = torch.utils.data.TensorDataset(*test_inputs)\n",
        "q_test_dataset = torch.utils.data.TensorDataset(*test_question_only_inputs)\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(fold_ids):\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    train_inputs = [inputs[i][train_idx] for i in range(3)]\n",
        "    q_train_inputs = [question_only_inputs[i][train_idx] for i in range(3)]\n",
        "    train_outputs = outputs[train_idx]\n",
        "    train_dataset = torch.utils.data.TensorDataset(*train_inputs, train_outputs)\n",
        "    q_train_dataset = torch.utils.data.TensorDataset(*q_train_inputs, train_outputs)\n",
        "\n",
        "    valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n",
        "    q_valid_inputs = [question_only_inputs[i][valid_idx] for i in range(3)]\n",
        "    valid_outputs = outputs[valid_idx]\n",
        "    valid_dataset = torch.utils.data.TensorDataset(*valid_inputs, valid_outputs)\n",
        "    q_valid_dataset = torch.utils.data.TensorDataset(*q_valid_inputs, valid_outputs)\n",
        "\n",
        "    history = train_and_predict(\n",
        "        train_data=train_dataset,\n",
        "        valid_data=valid_dataset,\n",
        "        test_data=test_dataset,\n",
        "        q_train_data=q_train_dataset,\n",
        "        q_valid_data=q_valid_dataset,\n",
        "        q_test_data=q_test_dataset,\n",
        "        q_epochs=3, epochs=3, batch_size=8, fold=fold\n",
        "        )\n",
        "\n",
        "    histories.append(history)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T15:27:36.986974Z",
          "iopub.execute_input": "2024-04-20T15:27:36.987302Z",
          "iopub.status.idle": "2024-04-20T16:54:09.419805Z",
          "shell.execute_reply.started": "2024-04-20T15:27:36.987246Z",
          "shell.execute_reply": "2024-04-20T16:54:09.419079Z"
        },
        "trusted": true,
        "id": "5UkKx_e7mhga",
        "outputId": "87f49fe4-ed0e-4461-8582-be5e2de87cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nEpoch 1: Train Loss 0.16246813971785798, Valid Loss 0.14927478905781696\n\t Train Spearmanr 0.2043, Valid Spearmanr (avg) 0.2838, Valid Spearmanr (last) 0.2816\n\t elapsed: 287.2598786354065s\n\nEpoch 2: Train Loss 0.1462297767459873, Valid Loss 0.1474814944925939\n\t Train Spearmanr 0.3080, Valid Spearmanr (avg) 0.2935, Valid Spearmanr (last) 0.2942\n\t elapsed: 286.3878049850464s\n\nEpoch 3: Train Loss 0.1398472028474013, Valid Loss 0.14674534547189794\n\t Train Spearmanr 0.3503, Valid Spearmanr (avg) 0.2982, Valid Spearmanr (last) 0.3017\n\t elapsed: 286.5851082801819s\n\nEpoch 1: Train Loss 0.17775589583944235, Valid Loss 0.1660405881557947\n\t Train Spearmanr 0.2690, Valid Spearmanr (avg) 0.4037, Valid Spearmanr (last) 0.3824\n\t elapsed: 286.0266637802124s\n\nEpoch 2: Train Loss 0.1627598503634097, Valid Loss 0.16378141110509287\n\t Train Spearmanr 0.4141, Valid Spearmanr (avg) 0.4116, Valid Spearmanr (last) 0.3984\n\t elapsed: 285.96063208580017s\n\nEpoch 3: Train Loss 0.15464559506388412, Valid Loss 0.16395867439095613\n\t Train Spearmanr 0.5017, Valid Spearmanr (avg) 0.4135, Valid Spearmanr (last) 0.4013\n\t elapsed: 286.08652925491333s\n\nEpoch 1: Train Loss 0.1616110454003016, Valid Loss 0.15076216732657802\n\t Train Spearmanr 0.2037, Valid Spearmanr (avg) 0.2960, Valid Spearmanr (last) 0.2885\n\t elapsed: 287.2682433128357s\n\nEpoch 2: Train Loss 0.1457214217478707, Valid Loss 0.14793903656362548\n\t Train Spearmanr 0.2920, Valid Spearmanr (avg) 0.2993, Valid Spearmanr (last) 0.3053\n\t elapsed: 287.12138509750366s\n\nEpoch 3: Train Loss 0.13876168531249966, Valid Loss 0.14757471936424887\n\t Train Spearmanr 0.3444, Valid Spearmanr (avg) 0.3062, Valid Spearmanr (last) 0.3055\n\t elapsed: 287.15662384033203s\n\nEpoch 1: Train Loss 0.17738412205989545, Valid Loss 0.16737025588985502\n\t Train Spearmanr 0.2634, Valid Spearmanr (avg) 0.4091, Valid Spearmanr (last) 0.3794\n\t elapsed: 286.8409118652344s\n\nEpoch 2: Train Loss 0.16245042334294177, Valid Loss 0.1644452908553007\n\t Train Spearmanr 0.4082, Valid Spearmanr (avg) 0.4203, Valid Spearmanr (last) 0.4080\n\t elapsed: 286.72010469436646s\n\nEpoch 3: Train Loss 0.15443944822284128, Valid Loss 0.16443061549949833\n\t Train Spearmanr 0.4971, Valid Spearmanr (avg) 0.4219, Valid Spearmanr (last) 0.4094\n\t elapsed: 286.83375811576843s\n\nEpoch 1: Train Loss 0.16094084922589508, Valid Loss 0.15085830307006837\n\t Train Spearmanr 0.2046, Valid Spearmanr (avg) 0.2807, Valid Spearmanr (last) 0.2771\n\t elapsed: 288.7264904975891s\n\nEpoch 2: Train Loss 0.14522480685629097, Valid Loss 0.149187433719635\n\t Train Spearmanr 0.2994, Valid Spearmanr (avg) 0.2860, Valid Spearmanr (last) 0.2848\n\t elapsed: 288.42439246177673s\n\nEpoch 3: Train Loss 0.13882239726828594, Valid Loss 0.1479597001671791\n\t Train Spearmanr 0.3440, Valid Spearmanr (avg) 0.2903, Valid Spearmanr (last) 0.2919\n\t elapsed: 288.5282757282257s\n\nEpoch 1: Train Loss 0.17749885826134215, Valid Loss 0.1683657795190811\n\t Train Spearmanr 0.2723, Valid Spearmanr (avg) 0.3878, Valid Spearmanr (last) 0.3609\n\t elapsed: 288.6479012966156s\n\nEpoch 2: Train Loss 0.1618503256609627, Valid Loss 0.16514489167928695\n\t Train Spearmanr 0.4158, Valid Spearmanr (avg) 0.3972, Valid Spearmanr (last) 0.3857\n\t elapsed: 287.7113878726959s\n\nEpoch 3: Train Loss 0.1543229650633008, Valid Loss 0.1650715330839157\n\t Train Spearmanr 0.4999, Valid Spearmanr (avg) 0.4010, Valid Spearmanr (last) 0.3919\n\t elapsed: 287.90911960601807s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Submit"
      ],
      "metadata": {
        "id": "J_rJYTUbmhga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get val preds per each epochs\n",
        "val_preds_list = []\n",
        "n_epochs = len(histories[0][0])\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    val_preds_one_epoch = np.zeros([len(df_train), 30])\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(fold_ids):\n",
        "        val_pred = histories[fold][0][epoch]\n",
        "        val_preds_one_epoch[valid_idx, :] += val_pred\n",
        "\n",
        "    val_preds_list.append(val_preds_one_epoch)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:54:09.421596Z",
          "iopub.execute_input": "2024-04-20T16:54:09.421861Z",
          "iopub.status.idle": "2024-04-20T16:54:09.434252Z",
          "shell.execute_reply.started": "2024-04-20T16:54:09.421812Z",
          "shell.execute_reply": "2024-04-20T16:54:09.433546Z"
        },
        "trusted": true,
        "id": "rn9M5UeNmhgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oof_predictions = np.zeros((n_epochs, len(df_train), len(output_categories)), dtype=np.float32)\n",
        "\n",
        "for j, name in enumerate(output_categories):\n",
        "    for epoch in range(n_epochs):\n",
        "        col = \"{}_{}\".format(epoch, name)\n",
        "        oof_predictions[epoch, :, j] = val_preds_list[epoch][:, j]\n",
        "\n",
        "oof_predictions.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:54:09.436140Z",
          "iopub.execute_input": "2024-04-20T16:54:09.436466Z",
          "iopub.status.idle": "2024-04-20T16:54:09.457433Z",
          "shell.execute_reply.started": "2024-04-20T16:54:09.436412Z",
          "shell.execute_reply": "2024-04-20T16:54:09.456772Z"
        },
        "trusted": true,
        "id": "k3fCvKkkmhgb",
        "outputId": "14c699c0-6ddf-4a92-cea0-dac27df280bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(6, 6079, 30)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get test preds per each epochs\n",
        "test_preds_list = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    test_preds_one_epoch = 0\n",
        "\n",
        "    for fold in range(len(fold_ids)):\n",
        "        test_preds = histories[fold][1][epoch]\n",
        "        test_preds_one_epoch += test_preds\n",
        "\n",
        "    test_preds_one_epoch = test_preds_one_epoch / len(fold_ids)\n",
        "    test_preds_list.append(test_preds_one_epoch)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:54:09.458746Z",
          "iopub.execute_input": "2024-04-20T16:54:09.458964Z",
          "iopub.status.idle": "2024-04-20T16:54:09.465015Z",
          "shell.execute_reply.started": "2024-04-20T16:54:09.458927Z",
          "shell.execute_reply": "2024-04-20T16:54:09.464379Z"
        },
        "trusted": true,
        "id": "GWuiDkfrmhgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = np.zeros((n_epochs, len(df_test), len(output_categories)), dtype=np.float32)\n",
        "\n",
        "for j, name in enumerate(output_categories):\n",
        "    for epoch in range(n_epochs):\n",
        "        col = \"{}_{}\".format(epoch, name)\n",
        "        test_predictions[epoch, :, j] = test_preds_list[epoch][:, j]\n",
        "\n",
        "test_predictions.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:54:09.466461Z",
          "iopub.execute_input": "2024-04-20T16:54:09.466751Z",
          "iopub.status.idle": "2024-04-20T16:54:09.479802Z",
          "shell.execute_reply.started": "2024-04-20T16:54:09.466704Z",
          "shell.execute_reply": "2024-04-20T16:54:09.479033Z"
        },
        "trusted": true,
        "id": "NX73YSLumhgc",
        "outputId": "ba708a5a-5e8e-4c3b-a254-8ad6a1282953"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(6, 476, 30)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from abc import abstractmethod\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "class Base_Model(object):\n",
        "    @abstractmethod\n",
        "    def fit(self, x_train, y_train, x_valid, y_valid, config):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_best_iteration(self, model):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, model, features):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_feature_importance(self, model):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def cv(self, y_train, train_features, test_features, feature_name, folds_ids, config):\n",
        "        # initialize\n",
        "        test_preds = np.zeros(len(test_features))\n",
        "        oof_preds = np.zeros(len(train_features))\n",
        "        importances = pd.DataFrame(index=feature_name)\n",
        "        best_iteration = 0\n",
        "        cv_score_list = []\n",
        "        models = []\n",
        "\n",
        "        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n",
        "            # get train data and valid data\n",
        "            x_trn = train_features.iloc[trn_idx]\n",
        "            y_trn = y_train[trn_idx]\n",
        "            x_val = train_features.iloc[val_idx]\n",
        "            y_val = y_train[val_idx]\n",
        "\n",
        "            # train model\n",
        "            model, best_score = self.fit(x_trn, y_trn, x_val, y_val, config)\n",
        "            cv_score_list.append(best_score)\n",
        "            models.append(model)\n",
        "            best_iteration += self.get_best_iteration(model) / len(folds_ids)\n",
        "\n",
        "            # predict out-of-fold and test\n",
        "            oof_preds[val_idx] = self.predict(model, x_val)\n",
        "            test_preds += self.predict(model, test_features) / len(folds_ids)\n",
        "\n",
        "            # get feature importances\n",
        "            importances_tmp = pd.DataFrame(\n",
        "                self.get_feature_importance(model),\n",
        "                columns=[f'gain_{i_fold+1}'],\n",
        "                index=feature_name\n",
        "            )\n",
        "            importances = importances.join(importances_tmp, how='inner')\n",
        "\n",
        "        # summary of feature importance\n",
        "        feature_importance = importances.mean(axis=1)\n",
        "\n",
        "        # full train\n",
        "        # model, best_score = self.full_train(train_features, y_train, config, best_iteration * 1.5)\n",
        "        # oof_preds = self.predict(model, train_features)\n",
        "        # test_preds = self.predict(model, test_features)\n",
        "\n",
        "        evals_results = {\"evals_result\": {\n",
        "            \"cv_score\": {f\"cv{i+1}\": cv_score for i, cv_score in enumerate(cv_score_list)},\n",
        "            \"n_data\": len(train_features),\n",
        "            \"best_iteration\": best_iteration,\n",
        "            \"n_features\": len(train_features.columns),\n",
        "            \"feature_importance\": feature_importance.sort_values(ascending=False).to_dict()\n",
        "        }}\n",
        "\n",
        "        return models, oof_preds, test_preds, feature_importance, evals_results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:54:09.481102Z",
          "iopub.execute_input": "2024-04-20T16:54:09.481422Z",
          "iopub.status.idle": "2024-04-20T16:54:09.499352Z",
          "shell.execute_reply.started": "2024-04-20T16:54:09.481368Z",
          "shell.execute_reply": "2024-04-20T16:54:09.498535Z"
        },
        "trusted": true,
        "id": "FjHV7uYWmhgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_compute_spearmanr(preds, trues):\n",
        "    rhos = spearmanr(trues.get_label(), preds).correlation\n",
        "    return \"spearmanr\", rhos, True\n",
        "\n",
        "\n",
        "def compute_spearmanr_each_col(trues, preds, n_bins=None):\n",
        "    if n_bins:\n",
        "        preds = binning_output(preds, n_bins)\n",
        "    rhos = spearmanr(trues, preds).correlation\n",
        "    return rhos"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:54:09.500785Z",
          "iopub.execute_input": "2024-04-20T16:54:09.501093Z",
          "iopub.status.idle": "2024-04-20T16:54:09.514509Z",
          "shell.execute_reply.started": "2024-04-20T16:54:09.501034Z",
          "shell.execute_reply": "2024-04-20T16:54:09.513759Z"
        },
        "trusted": true,
        "id": "KOGYetsimhgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class LightGBM(Base_Model):\n",
        "    def fit(self, x_train, y_train, x_valid, y_valid, config):\n",
        "        d_train = lgb.Dataset(x_train, label=y_train)\n",
        "        d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
        "        lgb_model_params = config[\"model\"][\"model_params\"]\n",
        "        lgb_train_params = config[\"model\"][\"train_params\"]\n",
        "        model = lgb.train(\n",
        "            params=lgb_model_params,\n",
        "            train_set=d_train,\n",
        "            valid_sets=[d_valid],\n",
        "            valid_names=['valid'],\n",
        "            feval=lgb_compute_spearmanr,\n",
        "            **lgb_train_params\n",
        "        )\n",
        "        best_score = dict(model.best_score)\n",
        "        return model, best_score\n",
        "\n",
        "    def full_train(self, x_train, y_train, config, iteration):\n",
        "        d_train = lgb.Dataset(x_train, label=y_train)\n",
        "        lgb_model_params = config[\"model\"][\"model_params\"]\n",
        "        model = lgb.train(\n",
        "            params=lgb_model_params,\n",
        "            train_set=d_train,\n",
        "            feval=lgb_compute_spearmanr,\n",
        "            num_boost_round=int(iteration)\n",
        "        )\n",
        "        best_score = dict(model.best_score)\n",
        "        return model, best_score\n",
        "\n",
        "    def get_best_iteration(self, model):\n",
        "        return model.best_iteration\n",
        "\n",
        "    def predict(self, model, features):\n",
        "        return model.predict(features)\n",
        "\n",
        "    def get_feature_importance(self, model):\n",
        "        return model.feature_importance(importance_type='gain')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:54:09.515987Z",
          "iopub.execute_input": "2024-04-20T16:54:09.516361Z",
          "iopub.status.idle": "2024-04-20T16:54:09.586816Z",
          "shell.execute_reply.started": "2024-04-20T16:54:09.516300Z",
          "shell.execute_reply": "2024-04-20T16:54:09.586281Z"
        },
        "trusted": true,
        "id": "uQ9KwX4wmhgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"model\": {\n",
        "        \"name\": \"lightgbm\",\n",
        "        \"model_params\": {\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"objective\": \"rmse\",\n",
        "            \"tree_learner\": \"serial\",\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"max_depth\": 1,\n",
        "            \"seed\": 71,\n",
        "            \"bagging_seed\": 71,\n",
        "            \"feature_fraction_seed\": 71,\n",
        "            \"drop_seed\": 71,\n",
        "            \"verbose\": -1\n",
        "        },\n",
        "        \"train_params\": {\n",
        "            \"num_boost_round\": 5000,\n",
        "            \"early_stopping_rounds\": 200,\n",
        "            \"verbose_eval\": 500\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "outputs = compute_output_arrays(df_train)\n",
        "oof_preds_list = []\n",
        "test_preds_list = []\n",
        "\n",
        "for i_col in range(len(output_categories)):\n",
        "    y_train = outputs[:, i_col]\n",
        "    #x_train = pd.DataFrame(oof_predictions[:, :, 2].T)\n",
        "    x_train = pd.DataFrame(np.concatenate([oof_predictions[:, :, i].T for i in range(30)], axis=1))\n",
        "    x_test = pd.DataFrame(np.concatenate([test_predictions[:, :, i].T for i in range(30)], axis=1))\n",
        "    feature_name = x_train.columns\n",
        "\n",
        "    model = LightGBM()\n",
        "    models, oof_preds, test_preds, feature_importance, evals_results = model.cv(\n",
        "            y_train, x_train, x_test, feature_name, fold_ids, config\n",
        "    )\n",
        "    oof_preds_list.append(oof_preds.reshape(-1, 1))\n",
        "    test_preds_list.append(test_preds.reshape(-1, 1))\n",
        "\n",
        "    print(i_col, output_categories[i_col])\n",
        "    print(compute_spearmanr_each_col(oof_preds, y_train))\n",
        "    print(len(oof_preds), len(np.unique(oof_preds)))\n",
        "    print(len(test_preds), len(np.unique(test_preds)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:54:09.587996Z",
          "iopub.execute_input": "2024-04-20T16:54:09.588259Z",
          "iopub.status.idle": "2024-04-20T16:55:15.423073Z",
          "shell.execute_reply.started": "2024-04-20T16:54:09.588212Z",
          "shell.execute_reply": "2024-04-20T16:55:15.422337Z"
        },
        "trusted": true,
        "id": "GbPi1lfKmhgd",
        "outputId": "036bdf73-13f9-4b1f-9bf0-bf218347e4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Training until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[39]\tvalid's rmse: 0.123569\tvalid's spearmanr: 0.361336\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[31]\tvalid's rmse: 0.124009\tvalid's spearmanr: 0.410875\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[14]\tvalid's rmse: 0.124894\tvalid's spearmanr: 0.395157\n0 question_asker_intent_understanding\n0.3788185409146311\n6079 1388\n476 379\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[21]\tvalid's rmse: 0.17538\tvalid's spearmanr: 0.640887\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[52]\tvalid's rmse: 0.168018\tvalid's spearmanr: 0.650678\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[32]\tvalid's rmse: 0.171639\tvalid's spearmanr: 0.613979\n1 question_body_critical\n0.623637700859402\n6079 803\n476 281\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[3]\tvalid's rmse: 0.164423\tvalid's spearmanr: 0.515272\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2]\tvalid's rmse: 0.18128\tvalid's spearmanr: 0.506473\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.179415\tvalid's spearmanr: 0.430969\n2 question_conversational\n0.34037797044021834\n6079 9\n476 5\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[40]\tvalid's rmse: 0.335798\tvalid's spearmanr: 0.323402\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[41]\tvalid's rmse: 0.331073\tvalid's spearmanr: 0.309812\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[20]\tvalid's rmse: 0.336533\tvalid's spearmanr: 0.297151\n3 question_expect_short_answer\n0.28901593908277334\n6079 1140\n476 377\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[15]\tvalid's rmse: 0.272666\tvalid's spearmanr: 0.381402\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[30]\tvalid's rmse: 0.278672\tvalid's spearmanr: 0.361174\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[4]\tvalid's rmse: 0.28372\tvalid's spearmanr: 0.316971\n4 question_fact_seeking\n0.2869148478852155\n6079 333\n476 95\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[8]\tvalid's rmse: 0.301033\tvalid's spearmanr: 0.47124\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[7]\tvalid's rmse: 0.31217\tvalid's spearmanr: 0.514642\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[6]\tvalid's rmse: 0.300469\tvalid's spearmanr: 0.478462\n5 question_has_commonly_accepted_answer\n0.37417164065294106\n6079 69\n476 34\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[22]\tvalid's rmse: 0.126439\tvalid's spearmanr: 0.357461\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[53]\tvalid's rmse: 0.129582\tvalid's spearmanr: 0.406446\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[9]\tvalid's rmse: 0.124676\tvalid's spearmanr: 0.337522\n6 question_interestingness_others\n0.3582520438037975\n6079 1393\n476 282\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[18]\tvalid's rmse: 0.161094\tvalid's spearmanr: 0.503469\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[16]\tvalid's rmse: 0.168139\tvalid's spearmanr: 0.543715\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[24]\tvalid's rmse: 0.156811\tvalid's spearmanr: 0.507984\n7 question_interestingness_self\n0.4964401695432705\n6079 509\n476 157\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[19]\tvalid's rmse: 0.26882\tvalid's spearmanr: 0.594103\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[18]\tvalid's rmse: 0.281912\tvalid's spearmanr: 0.57875\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[19]\tvalid's rmse: 0.272871\tvalid's spearmanr: 0.59412\n8 question_multi_intent\n0.5587606910797699\n6079 157\n476 98\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[3]\tvalid's rmse: 0.0418115\tvalid's spearmanr: 0.113986\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[3]\tvalid's rmse: 0.0520726\tvalid's spearmanr: 0.136895\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[29]\tvalid's rmse: 0.0421254\tvalid's spearmanr: 0.109451\n9 question_not_really_a_question\n0.07173516110414939\n6079 64\n476 37\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[59]\tvalid's rmse: 0.324748\tvalid's spearmanr: 0.447256\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[37]\tvalid's rmse: 0.315002\tvalid's spearmanr: 0.501955\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[41]\tvalid's rmse: 0.325651\tvalid's spearmanr: 0.422333\n10 question_opinion_seeking\n0.45306637905930064\n6079 1811\n476 426\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[13]\tvalid's rmse: 0.26316\tvalid's spearmanr: 0.75878\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[35]\tvalid's rmse: 0.247685\tvalid's spearmanr: 0.766226\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[15]\tvalid's rmse: 0.27684\tvalid's spearmanr: 0.727358\n11 question_type_choice\n0.7063171682171728\n6079 349\n476 162\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[10]\tvalid's rmse: 0.127123\tvalid's spearmanr: 0.5064\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[15]\tvalid's rmse: 0.130685\tvalid's spearmanr: 0.530761\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[5]\tvalid's rmse: 0.135368\tvalid's spearmanr: 0.462605\n12 question_type_compare\n0.2697147758800353\n6079 52\n476 21\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[8]\tvalid's rmse: 0.0758723\tvalid's spearmanr: 0.35075\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.0745173\tvalid's spearmanr: 0.25501\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[11]\tvalid's rmse: 0.0617283\tvalid's spearmanr: 0.290943\n13 question_type_consequence\n0.1545999122125165\n6079 24\n476 17\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.12374\tvalid's spearmanr: 0.600495\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[5]\tvalid's rmse: 0.125967\tvalid's spearmanr: 0.650698\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2]\tvalid's rmse: 0.132059\tvalid's spearmanr: 0.626624\n14 question_type_definition\n0.253449314733471\n6079 11\n476 6\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[7]\tvalid's rmse: 0.172668\tvalid's spearmanr: 0.594623\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[4]\tvalid's rmse: 0.168347\tvalid's spearmanr: 0.614989\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[6]\tvalid's rmse: 0.169275\tvalid's spearmanr: 0.556591\n15 question_type_entity\n0.3437016220289927\n6079 22\n476 13\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[16]\tvalid's rmse: 0.270041\tvalid's spearmanr: 0.790301\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[20]\tvalid's rmse: 0.259731\tvalid's spearmanr: 0.788721\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[14]\tvalid's rmse: 0.273782\tvalid's spearmanr: 0.787521\n16 question_type_instructions\n0.7661254586940125\n6079 266\n476 141\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[15]\tvalid's rmse: 0.242766\tvalid's spearmanr: 0.327236\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[49]\tvalid's rmse: 0.237184\tvalid's spearmanr: 0.357797\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[3]\tvalid's rmse: 0.25931\tvalid's spearmanr: 0.366403\n17 question_type_procedure\n0.31066511377998646\n6079 715\n476 214\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[41]\tvalid's rmse: 0.276845\tvalid's spearmanr: 0.68893\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[39]\tvalid's rmse: 0.274572\tvalid's spearmanr: 0.679336\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[16]\tvalid's rmse: 0.29078\tvalid's spearmanr: 0.656858\n18 question_type_reason_explanation\n0.6613212281944673\n6079 645\n476 268\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.0204935\tvalid's spearmanr: 0.631451\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n  c /= stddev[:, None]\n/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n  c /= stddev[None, :]\n/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n  return (a < x) & (x < b)\n/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n  return (a < x) & (x < b)\n/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n  cond2 = cond0 & (x <= _a)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.0245345\tvalid's spearmanr: nan\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\tvalid's rmse: 0.0147052\tvalid's spearmanr: 0.315515\n19 question_type_spelling\n0.011729285466170442\n6079 5\n476 2\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[39]\tvalid's rmse: 0.149061\tvalid's spearmanr: 0.527652\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[13]\tvalid's rmse: 0.158684\tvalid's spearmanr: 0.530721\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[36]\tvalid's rmse: 0.153778\tvalid's spearmanr: 0.505105\n20 question_well_written\n0.5021539539601263\n6079 1578\n476 409\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[20]\tvalid's rmse: 0.107337\tvalid's spearmanr: 0.232168\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[7]\tvalid's rmse: 0.114869\tvalid's spearmanr: 0.259303\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[8]\tvalid's rmse: 0.114173\tvalid's spearmanr: 0.217807\n21 answer_helpful\n0.20143567752649033\n6079 249\n476 123\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[22]\tvalid's rmse: 0.0954499\tvalid's spearmanr: 0.435658\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[36]\tvalid's rmse: 0.0996604\tvalid's spearmanr: 0.424891\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[76]\tvalid's rmse: 0.0989122\tvalid's spearmanr: 0.418474\n22 answer_level_of_information\n0.4210362665604529\n6079 1523\n476 325\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[23]\tvalid's rmse: 0.0835113\tvalid's spearmanr: 0.165617\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[5]\tvalid's rmse: 0.0890101\tvalid's spearmanr: 0.171764\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[41]\tvalid's rmse: 0.0852558\tvalid's spearmanr: 0.15834\n23 answer_plausible\n0.15014360846503344\n6079 623\n476 253\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[36]\tvalid's rmse: 0.0717831\tvalid's spearmanr: 0.156246\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[3]\tvalid's rmse: 0.0768772\tvalid's spearmanr: 0.205618\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[37]\tvalid's rmse: 0.0703192\tvalid's spearmanr: 0.203826\n24 answer_relevance\n0.15482124570312242\n6079 795\n476 282\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[32]\tvalid's rmse: 0.120351\tvalid's spearmanr: 0.302995\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[44]\tvalid's rmse: 0.123445\tvalid's spearmanr: 0.362735\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[20]\tvalid's rmse: 0.122979\tvalid's spearmanr: 0.303881\n25 answer_satisfaction\n0.31484470580657675\n6079 1404\n476 369\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[18]\tvalid's rmse: 0.28634\tvalid's spearmanr: 0.767087\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[31]\tvalid's rmse: 0.276442\tvalid's spearmanr: 0.750645\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[16]\tvalid's rmse: 0.291832\tvalid's spearmanr: 0.749293\n26 answer_type_instructions\n0.733847351339801\n6079 344\n476 157\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[27]\tvalid's rmse: 0.219397\tvalid's spearmanr: 0.277596\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[224]\tvalid's rmse: 0.21761\tvalid's spearmanr: 0.279119\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[7]\tvalid's rmse: 0.219819\tvalid's spearmanr: 0.284274\n27 answer_type_procedure\n0.26365252577150666\n6079 2208\n476 476\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[28]\tvalid's rmse: 0.304995\tvalid's spearmanr: 0.688575\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[33]\tvalid's rmse: 0.302652\tvalid's spearmanr: 0.67237\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[49]\tvalid's rmse: 0.305752\tvalid's spearmanr: 0.656491\n28 answer_type_reason_explanation\n0.6665974013791681\n6079 1427\n476 391\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[17]\tvalid's rmse: 0.0991144\tvalid's spearmanr: 0.23746\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[14]\tvalid's rmse: 0.099715\tvalid's spearmanr: 0.202552\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[28]\tvalid's rmse: 0.0952035\tvalid's spearmanr: 0.21464\n29 answer_well_written\n0.20623576725419485\n6079 226\n476 129\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_spearmanr(trues, preds, n_bins=None):\n",
        "    rhos = []\n",
        "    if n_bins:\n",
        "        preds = binning_output(preds, n_bins)\n",
        "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
        "        if len(np.unique(col_pred)) == 1:\n",
        "            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n",
        "        rhos.append(spearmanr(col_trues, col_pred).correlation)\n",
        "    return np.mean(rhos)\n",
        "\n",
        "\n",
        "oof_preds_fi = np.concatenate(oof_preds_list, axis=1)\n",
        "print(compute_spearmanr(outputs, oof_preds_fi))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T16:55:15.424649Z",
          "iopub.execute_input": "2024-04-20T16:55:15.424928Z",
          "iopub.status.idle": "2024-04-20T16:55:15.490383Z",
          "shell.execute_reply.started": "2024-04-20T16:55:15.424877Z",
          "shell.execute_reply": "2024-04-20T16:55:15.489637Z"
        },
        "trusted": true,
        "id": "mIAEEfh-mhge",
        "outputId": "46cb3110-ffe1-480f-e174-20d7cda785ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "0.37745278224662554\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_fi = np.concatenate(test_preds_list, axis=1)\n",
        "sub.iloc[:, 1:] = test_preds_fi\n",
        "sub.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T17:18:40.211779Z",
          "iopub.execute_input": "2024-04-20T17:18:40.212336Z",
          "iopub.status.idle": "2024-04-20T17:18:40.263772Z",
          "shell.execute_reply.started": "2024-04-20T17:18:40.212069Z",
          "shell.execute_reply": "2024-04-20T17:18:40.263129Z"
        },
        "trusted": true,
        "id": "xAR2lzOtmhge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDCeO8t7mhge"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}